# Daily Papers
The project automatically fetches the latest papers from arXiv based on keywords.

The subheadings in the README file represent the search keywords.

Only the most recent articles for each keyword are retained, up to a maximum of 100 papers.

You can click the 'Watch' button to receive daily email notifications.

Last update: 2025-05-06

## Efficient Diffusion Models
| **Title** | **Date** | **Abstract** | **Comment** |
| --- | --- | --- | --- |
| **[GENMO: A GENeralist Model for Human MOtion](http://arxiv.org/abs/2505.01425v1)** | 2025-05-02 | <details><summary>Show</summary><p>Human motion modeling traditionally separates motion generation and estimation into distinct tasks with specialized models. Motion generation models focus on creating diverse, realistic motions from inputs like text, audio, or keyframes, while motion estimation models aim to reconstruct accurate motion trajectories from observations like videos. Despite sharing underlying representations of temporal dynamics and kinematics, this separation limits knowledge transfer between tasks and requires maintaining separate models. We present GENMO, a unified Generalist Model for Human Motion that bridges motion estimation and generation in a single framework. Our key insight is to reformulate motion estimation as constrained motion generation, where the output motion must precisely satisfy observed conditioning signals. Leveraging the synergy between regression and diffusion, GENMO achieves accurate global motion estimation while enabling diverse motion generation. We also introduce an estimation-guided training objective that exploits in-the-wild videos with 2D annotations and text descriptions to enhance generative diversity. Furthermore, our novel architecture handles variable-length motions and mixed multimodal conditions (text, audio, video) at different time intervals, offering flexible control. This unified approach creates synergistic benefits: generative priors improve estimated motions under challenging conditions like occlusions, while diverse video data enhances generation capabilities. Extensive experiments demonstrate GENMO's effectiveness as a generalist framework that successfully handles multiple human motion tasks within a single model.</p></details> | <details><summary>Proje...</summary><p>Project page: https://research.nvidia.com/labs/dair/genmo/</p></details> |
| **[Wiki-TabNER: Integrating Named Entity Recognition into Wikipedia Tables](http://arxiv.org/abs/2403.04577v2)** | 2025-05-02 | <details><summary>Show</summary><p>Interest in solving table interpretation tasks has grown over the years, yet it still relies on existing datasets that may be overly simplified. This is potentially reducing the effectiveness of the dataset for thorough evaluation and failing to accurately represent tables as they appear in the real-world. To enrich the existing benchmark datasets, we extract and annotate a new, more challenging dataset. The proposed Wiki-TabNER dataset features complex tables containing several entities per cell, with named entities labeled using DBpedia classes. This dataset is specifically designed to address named entity recognition (NER) task within tables, but it can also be used as a more challenging dataset for evaluating the entity linking task. In this paper we describe the distinguishing features of the Wiki-TabNER dataset and the labeling process. In addition, we propose a prompting framework for evaluating the new large language models on the within tables NER task. Finally, we perform qualitative analysis to gain insights into the challenges encountered by the models and to understand the limitations of the proposed~dataset.</p></details> | <details><summary>Accep...</summary><p>Accepted at SIGIR 2025 conference</p></details> |
| **[Towards Optimal Deterministic LOCAL Algorithms on Trees](http://arxiv.org/abs/2505.01410v1)** | 2025-05-02 | <details><summary>Show</summary><p>While obtaining optimal algorithms for the most important problems in the LOCAL model has been one of the central goals in the area of distributed algorithms since its infancy, tight complexity bounds are elusive for many problems even when considering \emph{deterministic} complexities on \emph{trees}. We take a step towards remedying this issue by providing a way to relate the complexity of a problem $\Pi$ on trees to its truly local complexity, which is the (asymptotically) smallest function $f$ such that $\Pi$ can be solved in $O(f(\Delta)+\log^*n)$ rounds. More specifically, we develop a transformation that takes an algorithm $\mathcal A$ for $\Pi$ with a runtime of $O(f(\Delta)+\log^*n)$ rounds as input and transforms it into an $O(f(g(n))+\log^* n)$-round algorithm $\mathcal{A}'$ on trees, where $g$ is the function that satisfies $g(n)^{f(g(n))}=n$. If $f$ is the truly local complexity of $\Pi$ (i.e., if $\mathcal{A}$ is asymptotically optimal), then $\mathcal{A}'$ is an asymptotically optimal algorithm on trees, conditioned on a natural assumption on the nature of the worst-case instances of $\Pi$. Our transformation works for any member of a wide class of problems, including the most important symmetry-breaking problems. As an example of our transformation we obtain the first strongly sublogarithmic algorithm for $(\text{edge-degree+1})$-edge coloring (and therefore also $(2\Delta-1)$-edge coloring) on trees, exhibiting a runtime of $O(\log^{12/13} n)$ rounds. This breaks through the $\Omega(\log n/\log\log n)$-barrier that is a fundamental lower bound for other symmetry-breaking problems such as maximal independent set or maximal matching (that already holds on trees), and proves a separation between these problems and the aforementioned edge coloring problems on trees. We extend a subset of our results to graphs of bounded arboricity.</p></details> | <details><summary>Accep...</summary><p>Accepted at PODC 2025</p></details> |
| **[Bayesian Emulation of Grey-Box Multi-Model Ensembles Exploiting Known Interior Structure](http://arxiv.org/abs/2406.08367v2)** | 2025-05-02 | <details><summary>Show</summary><p>Computer models are widely used to study complex real world physical systems. However, there are major limitations to their direct use including: their complex structure; large numbers of inputs and outputs; and long evaluation times. Bayesian emulators are an effective means of addressing these challenges providing fast and efficient statistical approximation for computer model outputs. It is commonly assumed that computer models behave like a ``black-box'' function with no knowledge of the output prior to its evaluation. This ensures that emulators are generalisable but potentially limits their accuracy compared with exploiting such knowledge of constrained or structured output behaviour. We assume a ``grey-box'' computer model and develop a methodological toolkit for its analysis. This includes: multi-model ensemble subsampling to identifying a representative model subset to reduce computational expense; constructing a targeted Bayesian design for optimisation or decision support; a ``divide-and-conquer'' approach to emulating sums of outputs; structured emulators exploiting known constrained and structured behaviour of constituent outputs through splitting the parameter space and imposing truncations; emulation of sums of time series outputs; and emulation of multi-model ensemble outputs. Combining these methods establishes a hierarchical emulation framework which achieves greater physical interpretability and more accurate emulator predictions. This research is motivated by and applied to the commercially important TNO OLYMPUS Well Control Optimisation Challenge from the petroleum industry which we re-express as a decision support under uncertainty problem. We thus encourage users to examine their ``black-box'' simulators to achieve superior emulator accuracy.</p></details> | 50 pages, 15 figures |
| **[Dynamic Robot Tool Use with Vision Language Models](http://arxiv.org/abs/2505.01399v1)** | 2025-05-02 | <details><summary>Show</summary><p>Tool use enhances a robot's task capabilities. Recent advances in vision-language models (VLMs) have equipped robots with sophisticated cognitive capabilities for tool-use applications. However, existing methodologies focus on elementary quasi-static tool manipulations or high-level tool selection while neglecting the critical aspect of task-appropriate tool grasping. To address this limitation, we introduce inverse Tool-Use Planning (iTUP), a novel VLM-driven framework that enables grounded fine-grained planning for versatile robotic tool use. Through an integrated pipeline of VLM-based tool and contact point grounding, position-velocity trajectory planning, and physics-informed grasp generation and selection, iTUP demonstrates versatility across (1) quasi-static and more challenging (2) dynamic and (3) cluster tool-use tasks. To ensure robust planning, our framework integrates stable and safe task-aware grasping by reasoning over semantic affordances and physical constraints. We evaluate iTUP and baselines on a comprehensive range of realistic tool use tasks including precision hammering, object scooping, and cluster sweeping. Experimental results demonstrate that iTUP ensures a thorough grounding of cognition and planning for challenging robot tool use across diverse environments.</p></details> | <details><summary>In su...</summary><p>In submission and under review</p></details> |
| **[An Automated Pipeline for Few-Shot Bird Call Classification: A Case Study with the Tooth-Billed Pigeon](http://arxiv.org/abs/2504.16276v2)** | 2025-05-02 | <details><summary>Show</summary><p>This paper presents an automated one-shot bird call classification pipeline designed for rare species absent from large publicly available classifiers like BirdNET and Perch. While these models excel at detecting common birds with abundant training data, they lack options for species with only 1-3 known recordings-a critical limitation for conservationists monitoring the last remaining individuals of endangered birds. To address this, we leverage the embedding space of large bird classification networks and develop a classifier using cosine similarity, combined with filtering and denoising preprocessing techniques, to optimize detection with minimal training data. We evaluate various embedding spaces using clustering metrics and validate our approach in both a simulated scenario with Xeno-Canto recordings and a real-world test on the critically endangered tooth-billed pigeon (Didunculus strigirostris), which has no existing classifiers and only three confirmed recordings. The final model achieved 1.0 recall and 0.95 accuracy in detecting tooth-billed pigeon calls, making it practical for use in the field. This open-source system provides a practical tool for conservationists seeking to detect and monitor rare species on the brink of extinction.</p></details> | <details><summary>16 pa...</summary><p>16 pages, 5 figures, 4 tables</p></details> |
| **[An Efficient Matrix Multiplication Algorithm for Accelerating Inference in Binary and Ternary Neural Networks](http://arxiv.org/abs/2411.06360v3)** | 2025-05-02 | <details><summary>Show</summary><p>Despite their tremendous success and versatility, Deep Neural Networks (DNNs) such as Large Language Models (LLMs) suffer from inference inefficiency and rely on advanced computational infrastructure. To address these challenges and make these models more accessible and cost-effective, in this paper, we propose algorithms to improve the inference time and memory efficiency of DNNs with binary and ternary weight matrices. Particularly focusing on matrix multiplication as the bottleneck operation of inference, we observe that, once trained, the weight matrices of a model no longer change. This allows us to preprocess these matrices and create indices that help reduce the storage requirements by a logarithmic factor while enabling our efficient inference algorithms. Specifically, for a $n\times n$ weight matrix, our efficient algorithm guarantees a time complexity of $O(\frac{n^2}{\log n})$, a logarithmic factor improvement over the standard vector-matrix multiplication. Besides theoretical analysis, we conduct extensive experiments to evaluate the practical efficiency of our algorithms. Our results confirm the superiority of our approach both with respect to time and memory, as we observed a reduction in the multiplication time up to 29x and memory usage up to 6x. When applied to LLMs, our experiments show up to a 5.24x speedup in the inference time.</p></details> | <details><summary>Accep...</summary><p>Accepted at ICML 2025</p></details> |
| **[Multimodal Doctor-in-the-Loop: A Clinically-Guided Explainable Framework for Predicting Pathological Response in Non-Small Cell Lung Cancer](http://arxiv.org/abs/2505.01390v1)** | 2025-05-02 | <details><summary>Show</summary><p>This study proposes a novel approach combining Multimodal Deep Learning with intrinsic eXplainable Artificial Intelligence techniques to predict pathological response in non-small cell lung cancer patients undergoing neoadjuvant therapy. Due to the limitations of existing radiomics and unimodal deep learning approaches, we introduce an intermediate fusion strategy that integrates imaging and clinical data, enabling efficient interaction between data modalities. The proposed Multimodal Doctor-in-the-Loop method further enhances clinical relevance by embedding clinicians' domain knowledge directly into the training process, guiding the model's focus gradually from broader lung regions to specific lesions. Results demonstrate improved predictive accuracy and explainability, providing insights into optimal data integration strategies for clinical applications.</p></details> | <details><summary>arXiv...</summary><p>arXiv admin note: substantial text overlap with arXiv:2502.17503</p></details> |
| **[chebgreen: Learning and Interpolating Continuous Empirical Green's Functions from Data](http://arxiv.org/abs/2501.18715v3)** | 2025-05-02 | <details><summary>Show</summary><p>In this work, we present a mesh-independent, data-driven library, chebgreen, to mathematically model one-dimensional systems, possessing an associated control parameter, and whose governing partial differential equation is unknown. The proposed method learns an Empirical Green's Function for the associated, but hidden, boundary value problem, in the form of a Rational Neural Network from which we subsequently construct a bivariate representation in a Chebyshev basis. We uncover the Green's function, at an unseen control parameter value, by interpolating the left and right singular functions within a suitable library, expressed as points on a manifold of Quasimatrices, while the associated singular values are interpolated with Lagrange polynomials.</p></details> | <details><summary>Code ...</summary><p>Code is available at https://github.com/hsharsh/chebgreen</p></details> |
| **[An Efficient Real-Time Planning Method for Swarm Robotics Based on an Optimal Virtual Tube](http://arxiv.org/abs/2505.01380v1)** | 2025-05-02 | <details><summary>Show</summary><p>Swarm robotics navigating through unknown obstacle environments is an emerging research area that faces challenges. Performing tasks in such environments requires swarms to achieve autonomous localization, perception, decision-making, control, and planning. The limited computational resources of onboard platforms present significant challenges for planning and control. Reactive planners offer low computational demands and high re-planning frequencies but lack predictive capabilities, often resulting in local minima. Long-horizon planners, on the other hand, can perform multi-step predictions to reduce deadlocks but cost much computation, leading to lower re-planning frequencies. This paper proposes a real-time optimal virtual tube planning method for swarm robotics in unknown environments, which generates approximate solutions for optimal trajectories through affine functions. As a result, the computational complexity of approximate solutions is $O(n_t)$, where $n_t$ is the number of parameters in the trajectory, thereby significantly reducing the overall computational burden. By integrating reactive methods, the proposed method enables low-computation, safe swarm motion in unknown environments. The effectiveness of the proposed method is validated through several simulations and experiments.</p></details> | 18 pages, 21 figures |
| **[SimICD: A Closed-Loop Simulation Framework For ICD Therapy](http://arxiv.org/abs/2505.01371v1)** | 2025-05-02 | <details><summary>Show</summary><p>Virtual studies of ICD behaviour are crucial for testing device functionality in a controlled environment prior to clinical application. Although previous works have shown the viability of using in silico testing for diagnosis, there is a notable gap in available models that can simulate therapy progression decisions during arrhythmic episodes. This work introduces SimICD, a simulation tool which combines virtual ICD logic algorithms with cardiac electrophysiology simulations in a feedback loop, allowing the progression of ICD therapy protocols to be simulated for a range of tachy-arrhythmia episodes. Using a cohort of virtual patients, we demonstrate the ability of SimICD to simulate realistic cardiac signals and ICD responses that align with the logic of real-world devices, facilitating the reprogramming of ICD parameters to adapt to specific episodes.</p></details> | <details><summary>Accep...</summary><p>Accepted for publication in the 47th annual Engineering in Medicine and Biology Conference (EMBC)</p></details> |
| **[Binamix -- A Python Library for Generating Binaural Audio Datasets](http://arxiv.org/abs/2505.01369v1)** | 2025-05-02 | <details><summary>Show</summary><p>The increasing demand for spatial audio in applications such as virtual reality, immersive media, and spatial audio research necessitates robust solutions to generate binaural audio data sets for use in testing and validation. Binamix is an open-source Python library designed to facilitate programmatic binaural mixing using the extensive SADIE II Database, which provides Head Related Impulse Response (HRIR) and Binaural Room Impulse Response (BRIR) data for 20 subjects. The Binamix library provides a flexible and repeatable framework for creating large-scale spatial audio datasets, making it an invaluable resource for codec evaluation, audio quality metric development, and machine learning model training. A range of pre-built example scripts, utility functions, and visualization plots further streamline the process of custom pipeline creation. This paper presents an overview of the library's capabilities, including binaural rendering, impulse response interpolation, and multi-track mixing for various speaker layouts. The tools utilize a modified Delaunay triangulation technique to achieve accurate HRIR/BRIR interpolation where desired angles are not present in the data. By supporting a wide range of parameters such as azimuth, elevation, subject Impulse Responses (IRs), speaker layouts, mixing controls, and more, the library enables researchers to create large binaural datasets for any downstream purpose. Binamix empowers researchers and developers to advance spatial audio applications with reproducible methodologies by offering an open-source solution for binaural rendering and dataset generation. We release the library under the Apache 2.0 License at https://github.com/QxLabIreland/Binamix/</p></details> | <details><summary>Accep...</summary><p>Accepted to the 158th Audio Engineering Society Convention, 2025</p></details> |
| **[Project-and-Fuse: Improving RGB-D Semantic Segmentation via Graph Convolution Networks](http://arxiv.org/abs/2501.18851v3)** | 2025-05-02 | <details><summary>Show</summary><p>Most existing RGB-D semantic segmentation methods focus on the feature level fusion, including complex cross-modality and cross-scale fusion modules. However, these methods may cause misalignment problem in the feature fusion process and counter-intuitive patches in the segmentation results. Inspired by the popular pixel-node-pixel pipeline, we propose to 1) fuse features from two modalities in a late fusion style, during which the geometric feature injection is guided by texture feature prior; 2) employ Graph Neural Networks (GNNs) on the fused feature to alleviate the emergence of irregular patches by inferring patch relationship. At the 3D feature extraction stage, we argue that traditional CNNs are not efficient enough for depth maps. So, we encode depth map into normal map, after which CNNs can easily extract object surface tendencies.At projection matrix generation stage, we find the existence of Biased-Assignment and Ambiguous-Locality issues in the original pipeline. Therefore, we propose to 1) adopt the Kullback-Leibler Loss to ensure no missing important pixel features, which can be viewed as hard pixel mining process; 2) connect regions that are close to each other in the Euclidean space as well as in the semantic space with larger edge weights so that location informations can been considered. Extensive experiments on two public datasets, NYU-DepthV2 and SUN RGB-D, have shown that our approach can consistently boost the performance of RGB-D semantic segmentation task.</p></details> | <details><summary>I hav...</summary><p>I have decided to withdraw this paper because I have recently obtained some new data and insights during my ongoing research</p></details> |
| **[Exponential Quantum Advantage for Pathfinding in Regular Sunflower Graphs](http://arxiv.org/abs/2407.14398v2)** | 2025-05-02 | <details><summary>Show</summary><p>Finding problems that allow for superpolynomial quantum speedup is one of the most important tasks in quantum computation. A key challenge is identifying problem structures that can only be exploited by quantum mechanics. In this paper, we find a class of graphs that allows for exponential quantum-classical separation for the pathfinding problem with the adjacency list oracle, and this class of graphs is named regular sunflower graphs. We prove that, with high probability, a regular sunflower graph of degree at least $7$ is a mild expander graph, that is, the spectral gap of the graph Laplacian is at least inverse polylogarithmic in the graph size. We provide an efficient quantum algorithm to find an $s$-$t$ path in the regular sunflower graph while any classical algorithm takes exponential time. This quantum advantage is achieved by efficiently preparing a $0$-eigenstate of the adjacency matrix of the regular sunflower graph as a quantum superposition state over the vertices, and this quantum state contains enough information to help us efficiently find an $s$-$t$ path in the regular sunflower graph. Because the security of an isogeny-based cryptosystem depends on the hardness of finding an $s$-$t$ path in an expander graph \cite{Charles2009}, a quantum speedup of the pathfinding problem on an expander graph is of significance. Our result represents a step towards this goal as the first provable exponential speedup for pathfinding in a mild expander graph.</p></details> | 45 pages,3 figures |
| **[Offline Model-Based Optimization by Learning to Rank](http://arxiv.org/abs/2410.11502v3)** | 2025-05-02 | <details><summary>Show</summary><p>Offline model-based optimization (MBO) aims to identify a design that maximizes a black-box function using only a fixed, pre-collected dataset of designs and their corresponding scores. A common approach in offline MBO is to train a regression-based surrogate model by minimizing mean squared error (MSE) and then find the best design within this surrogate model by different optimizers (e.g., gradient ascent). However, a critical challenge is the risk of out-of-distribution errors, i.e., the surrogate model may typically overestimate the scores and mislead the optimizers into suboptimal regions. Prior works have attempted to address this issue in various ways, such as using regularization techniques and ensemble learning to enhance the robustness of the model, but it still remains. In this paper, we argue that regression models trained with MSE are not well-aligned with the primary goal of offline MBO, which is to select promising designs rather than to predict their scores precisely. Notably, if a surrogate model can maintain the order of candidate designs based on their relative score relationships, it can produce the best designs even without precise predictions. To validate it, we conduct experiments to compare the relationship between the quality of the final designs and MSE, finding that the correlation is really very weak. In contrast, a metric that measures order-maintaining quality shows a significantly stronger correlation. Based on this observation, we propose learning a ranking-based model that leverages learning to rank techniques to prioritize promising designs based on their relative scores. We show that the generalization error on ranking loss can be well bounded. Empirical results across diverse tasks demonstrate the superior performance of our proposed ranking-based models than twenty existing methods.</p></details> | ICLR 2025 |
| **[Differentiable Nonlinear Model Predictive Control](http://arxiv.org/abs/2505.01353v1)** | 2025-05-02 | <details><summary>Show</summary><p>The efficient computation of parametric solution sensitivities is a key challenge in the integration of learning-enhanced methods with nonlinear model predictive control (MPC), as their availability is crucial for many learning algorithms. While approaches presented in the machine learning community are limited to convex or unconstrained formulations, this paper discusses the computation of solution sensitivities of general nonlinear programs (NLPs) using the implicit function theorem (IFT) and smoothed optimality conditions treated in interior-point methods (IPM). We detail sensitivity computation within a sequential quadratic programming (SQP) method which employs an IPM for the quadratic subproblems. The publication is accompanied by an efficient open-source implementation within the framework, providing both forward and adjoint sensitivities for general optimal control problems, achieving speedups exceeding 3x over the state-of-the-art solver mpc.pytorch.</p></details> | <details><summary>19 pa...</summary><p>19 page, 4 figures, 2 tables</p></details> |
| **[MoDeGPT: Modular Decomposition for Large Language Model Compression](http://arxiv.org/abs/2408.09632v5)** | 2025-05-02 | <details><summary>Show</summary><p>Large Language Models (LLMs) have reshaped the landscape of artificial intelligence by demonstrating exceptional performance across various tasks. However, substantial computational requirements make their deployment challenging on devices with limited resources. Recently, compression methods using low-rank matrix techniques have shown promise, yet these often lead to degraded accuracy or introduce significant overhead in parameters and inference latency. This paper introduces \textbf{Mo}dular \textbf{De}composition (MoDeGPT), a novel structured compression framework that does not need recovery fine-tuning while resolving the above drawbacks. MoDeGPT partitions the Transformer block into modules comprised of matrix pairs and reduces the hidden dimensions via reconstructing the module-level outputs. MoDeGPT is developed based on a theoretical framework that utilizes three well-established matrix decomposition algorithms -- Nystr\"om approximation, CR decomposition, and SVD -- and applies them to our redefined transformer modules. Our comprehensive experiments show MoDeGPT, without backward propagation, matches or surpasses previous structured compression methods that rely on gradient information, and saves 98% of compute costs on compressing a 13B model. On \textsc{Llama}-2/3 and OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30% compression rates. Moreover, the compression can be done on a single GPU within a few hours and increases the inference throughput by up to 46%.</p></details> | ICLR 2025 Oral |
| **[How to Learn a Star: Binary Classification with Starshaped Polyhedral Sets](http://arxiv.org/abs/2505.01346v1)** | 2025-05-02 | <details><summary>Show</summary><p>We consider binary classification restricted to a class of continuous piecewise linear functions whose decision boundaries are (possibly nonconvex) starshaped polyhedral sets, supported on a fixed polyhedral simplicial fan. We investigate the expressivity of these function classes and describe the combinatorial and geometric structure of the loss landscape, most prominently the sublevel sets, for two loss-functions: the 0/1-loss (discrete loss) and an exponential loss function. In particular, we give explicit bounds on the VC dimension of this model, and concretely describe the sublevel sets of the discrete loss as chambers in a hyperplane arrangement. For the exponential loss, we give sufficient conditions for the optimum to be unique, and describe the geometry of the optimum when varying the rate parameter of the underlying exponential probability distribution.</p></details> | 22 pages, 8 figures |
| **[Integration of Multi-Mode Preference into Home Energy Management System Using Deep Reinforcement Learning](http://arxiv.org/abs/2505.01332v1)** | 2025-05-02 | <details><summary>Show</summary><p>Home Energy Management Systems (HEMS) have emerged as a pivotal tool in the smart home ecosystem, aiming to enhance energy efficiency, reduce costs, and improve user comfort. By enabling intelligent control and optimization of household energy consumption, HEMS plays a significant role in bridging the gap between consumer needs and energy utility objectives. However, much of the existing literature construes consumer comfort as a mere deviation from the standard appliance settings. Such deviations are typically incorporated into optimization objectives via static weighting factors. These factors often overlook the dynamic nature of consumer behaviors and preferences. Addressing this oversight, our paper introduces a multi-mode Deep Reinforcement Learning-based HEMS (DRL-HEMS) framework, meticulously designed to optimize based on dynamic, consumer-defined preferences. Our primary goal is to augment consumer involvement in Demand Response (DR) programs by embedding dynamic multi-mode preferences tailored to individual appliances. In this study, we leverage a model-free, single-agent DRL algorithm to deliver a HEMS framework that is not only dynamic but also user-friendly. To validate its efficacy, we employed real-world data at 15-minute intervals, including metrics such as electricity price, ambient temperature, and appliances' power consumption. Our results show that the model performs exceptionally well in optimizing energy consumption within different preference modes. Furthermore, when compared to traditional algorithms based on Mixed-Integer Linear Programming (MILP), our model achieves nearly optimal performance while outperforming in computational efficiency.</p></details> | <details><summary>Accep...</summary><p>Accepted for publication in ASME journal of engineering for sustainable buildings and cities</p></details> |
| **[TRAVELER: A Benchmark for Evaluating Temporal Reasoning across Vague, Implicit and Explicit References](http://arxiv.org/abs/2505.01325v1)** | 2025-05-02 | <details><summary>Show</summary><p>Understanding and resolving temporal references is essential in Natural Language Understanding as we often refer to the past or future in daily communication. Although existing benchmarks address a system's ability to reason about and resolve temporal references, systematic evaluation of specific temporal references remains limited. Towards closing this gap, we introduce TRAVELER, a novel synthetic benchmark dataset that follows a Question Answering paradigm and consists of questions involving temporal references with the corresponding correct answers. TRAVELER assesses models' abilities to resolve explicit, implicit relative to speech time, and vague temporal references. Beyond investigating the performance of state-of-the-art LLMs depending on the type of temporal reference, our benchmark also allows evaluation of performance in relation to the length of the set of events. For the category of vague temporal references, ground-truth answers were established via human surveys on Prolific, following a procedure similar to the one from Kenneweg et al. To demonstrate the benchmark's applicability, we evaluate four state-of-the-art LLMs using a question-answering task encompassing 3,300 questions. Our findings show that while the benchmarked LLMs can answer questions over event sets with a handful of events and explicit temporal references successfully, performance clearly deteriorates with larger event set length and when temporal references get less explicit. Notably, the vague question category exhibits the lowest performance across all models. The benchmark is publicly available at: https://gitlab.ub.uni-bielefeld.de/s.kenneweg/TRAVELER</p></details> | <details><summary>24 pa...</summary><p>24 pages, 6 figures, submitted to Springer Nature Computer Science</p></details> |
| **[Design-Based Inference under Random Potential Outcomes via Riesz Representation](http://arxiv.org/abs/2505.01324v1)** | 2025-05-02 | <details><summary>Show</summary><p>We introduce a general framework for design-based causal inference that accommodates stochastic potential outcomes, thereby extending the classical Neyman-Rubin setup in which outcomes are treated as fixed. In our formulation, each unit's potential outcome is modelled as a function $\tilde{y}_i(z, \omega)$, where $\omega$ denotes latent randomness external to the treatment assignment. Building on recent work that connects design-based estimation with the Riesz representation theorem, we construct causal estimators by embedding potential outcomes in a Hilbert space and defining treatment effects as linear functionals. This allows us to derive unbiased and consistent estimators, even when potential outcomes exhibit random variation. The framework retains the key advantage of design-based analysis, namely, the use of a known randomisation scheme for identification, while enabling inference in settings with inherent stochasticity. We establish large-sample properties under local dependence, provide a variance estimator compatible with sparse dependency structures, and illustrate the method through a simulation. Our results unify design-based reasoning with random-outcome modelling, broadening the applicability of causal inference in complex experimental environments.</p></details> | <details><summary>37 pa...</summary><p>37 pages, 2 figures, 2 Tables, 2 Algorithms. Preprint prepared for journal submission</p></details> |
| **[Model See Model Do: Speech-Driven Facial Animation with Style Control](http://arxiv.org/abs/2505.01319v1)** | 2025-05-02 | <details><summary>Show</summary><p>Speech-driven 3D facial animation plays a key role in applications such as virtual avatars, gaming, and digital content creation. While existing methods have made significant progress in achieving accurate lip synchronization and generating basic emotional expressions, they often struggle to capture and effectively transfer nuanced performance styles. We propose a novel example-based generation framework that conditions a latent diffusion model on a reference style clip to produce highly expressive and temporally coherent facial animations. To address the challenge of accurately adhering to the style reference, we introduce a novel conditioning mechanism called style basis, which extracts key poses from the reference and additively guides the diffusion generation process to fit the style without compromising lip synchronization quality. This approach enables the model to capture subtle stylistic cues while ensuring that the generated animations align closely with the input speech. Extensive qualitative, quantitative, and perceptual evaluations demonstrate the effectiveness of our method in faithfully reproducing the desired style while achieving superior lip synchronization across various speech scenarios.</p></details> | <details><summary>10 pa...</summary><p>10 pages, 7 figures, SIGGRAPH Conference Papers '25</p></details> |

## Transformer Compression
| **Title** | **Date** | **Abstract** | **Comment** |
| --- | --- | --- | --- |
| **[Towards Optimal Deterministic LOCAL Algorithms on Trees](http://arxiv.org/abs/2505.01410v1)** | 2025-05-02 | <details><summary>Show</summary><p>While obtaining optimal algorithms for the most important problems in the LOCAL model has been one of the central goals in the area of distributed algorithms since its infancy, tight complexity bounds are elusive for many problems even when considering \emph{deterministic} complexities on \emph{trees}. We take a step towards remedying this issue by providing a way to relate the complexity of a problem $\Pi$ on trees to its truly local complexity, which is the (asymptotically) smallest function $f$ such that $\Pi$ can be solved in $O(f(\Delta)+\log^*n)$ rounds. More specifically, we develop a transformation that takes an algorithm $\mathcal A$ for $\Pi$ with a runtime of $O(f(\Delta)+\log^*n)$ rounds as input and transforms it into an $O(f(g(n))+\log^* n)$-round algorithm $\mathcal{A}'$ on trees, where $g$ is the function that satisfies $g(n)^{f(g(n))}=n$. If $f$ is the truly local complexity of $\Pi$ (i.e., if $\mathcal{A}$ is asymptotically optimal), then $\mathcal{A}'$ is an asymptotically optimal algorithm on trees, conditioned on a natural assumption on the nature of the worst-case instances of $\Pi$. Our transformation works for any member of a wide class of problems, including the most important symmetry-breaking problems. As an example of our transformation we obtain the first strongly sublogarithmic algorithm for $(\text{edge-degree+1})$-edge coloring (and therefore also $(2\Delta-1)$-edge coloring) on trees, exhibiting a runtime of $O(\log^{12/13} n)$ rounds. This breaks through the $\Omega(\log n/\log\log n)$-barrier that is a fundamental lower bound for other symmetry-breaking problems such as maximal independent set or maximal matching (that already holds on trees), and proves a separation between these problems and the aforementioned edge coloring problems on trees. We extend a subset of our results to graphs of bounded arboricity.</p></details> | <details><summary>Accep...</summary><p>Accepted at PODC 2025</p></details> |
| **[MoDeGPT: Modular Decomposition for Large Language Model Compression](http://arxiv.org/abs/2408.09632v5)** | 2025-05-02 | <details><summary>Show</summary><p>Large Language Models (LLMs) have reshaped the landscape of artificial intelligence by demonstrating exceptional performance across various tasks. However, substantial computational requirements make their deployment challenging on devices with limited resources. Recently, compression methods using low-rank matrix techniques have shown promise, yet these often lead to degraded accuracy or introduce significant overhead in parameters and inference latency. This paper introduces \textbf{Mo}dular \textbf{De}composition (MoDeGPT), a novel structured compression framework that does not need recovery fine-tuning while resolving the above drawbacks. MoDeGPT partitions the Transformer block into modules comprised of matrix pairs and reduces the hidden dimensions via reconstructing the module-level outputs. MoDeGPT is developed based on a theoretical framework that utilizes three well-established matrix decomposition algorithms -- Nystr\"om approximation, CR decomposition, and SVD -- and applies them to our redefined transformer modules. Our comprehensive experiments show MoDeGPT, without backward propagation, matches or surpasses previous structured compression methods that rely on gradient information, and saves 98% of compute costs on compressing a 13B model. On \textsc{Llama}-2/3 and OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30% compression rates. Moreover, the compression can be done on a single GPU within a few hours and increases the inference throughput by up to 46%.</p></details> | ICLR 2025 Oral |
| **[A Transformer-based Neural Architecture Search Method](http://arxiv.org/abs/2505.01314v1)** | 2025-05-02 | <details><summary>Show</summary><p>This paper presents a neural architecture search method based on Transformer architecture, searching cross multihead attention computation ways for different number of encoder and decoder combinations. In order to search for neural network structures with better translation results, we considered perplexity as an auxiliary evaluation metric for the algorithm in addition to BLEU scores and iteratively improved each individual neural network within the population by a multi-objective genetic algorithm. Experimental results show that the neural network structures searched by the algorithm outperform all the baseline models, and that the introduction of the auxiliary evaluation metric can find better models than considering only the BLEU score as an evaluation metric.</p></details> | GECCO 2023 |
| **[2DXformer: Dual Transformers for Wind Power Forecasting with Dual Exogenous Variables](http://arxiv.org/abs/2505.01286v1)** | 2025-05-02 | <details><summary>Show</summary><p>Accurate wind power forecasting can help formulate scientific dispatch plans, which is of great significance for maintaining the safety, stability, and efficient operation of the power system. In recent years, wind power forecasting methods based on deep learning have focused on extracting the spatiotemporal correlations among data, achieving significant improvements in forecasting accuracy. However, they exhibit two limitations. First, there is a lack of modeling for the inter-variable relationships, which limits the accuracy of the forecasts. Second, by treating endogenous and exogenous variables equally, it leads to unnecessary interactions between the endogenous and exogenous variables, increasing the complexity of the model. In this paper, we propose the 2DXformer, which, building upon the previous work's focus on spatiotemporal correlations, addresses the aforementioned two limitations. Specifically, we classify the inputs of the model into three types: exogenous static variables, exogenous dynamic variables, and endogenous variables. First, we embed these variables as variable tokens in a channel-independent manner. Then, we use the attention mechanism to capture the correlations among exogenous variables. Finally, we employ a multi-layer perceptron with residual connections to model the impact of exogenous variables on endogenous variables. Experimental results on two real-world large-scale datasets indicate that our proposed 2DXformer can further improve the performance of wind power forecasting. The code is available in this repository: \href{https://github.com/jseaj/2DXformer}{https://github.com/jseaj/2DXformer}.</p></details> | <details><summary>Accep...</summary><p>Accepted by ICDM 2024</p></details> |
| **[Fusing Foveal Fixations Using Linear Retinal Transformations and Bayesian Experimental Design](http://arxiv.org/abs/2505.01249v1)** | 2025-05-02 | <details><summary>Show</summary><p>Humans (and many vertebrates) face the problem of fusing together multiple fixations of a scene in order to obtain a representation of the whole, where each fixation uses a high-resolution fovea and decreasing resolution in the periphery. In this paper we explicitly represent the retinal transformation of a fixation as a linear downsampling of a high-resolution latent image of the scene, exploiting the known geometry. This linear transformation allows us to carry out exact inference for the latent variables in factor analysis (FA) and mixtures of FA models of the scene. Further, this allows us to formulate and solve the choice of "where to look next" as a Bayesian experimental design problem using the Expected Information Gain criterion. Experiments on the Frey faces and MNIST datasets demonstrate the effectiveness of our models.</p></details> | 19 pages, 4 figures |
| **[Asymptotic Linear Convergence of ADMM for Isotropic TV Norm Compressed Sensing](http://arxiv.org/abs/2505.01240v1)** | 2025-05-02 | <details><summary>Show</summary><p>We prove an explicit local linear rate for ADMM solving the isotropic Total Variation (TV) norm compressed sensing problem in multiple dimensions, by analyzing the auxiliary variable in the equivalent Douglas-Rachford splitting on a dual problem. Numerical verification on large 3D problems and real MRI data will be shown. Though the proven rate is not sharp, it is close to the observed ones in numerical tests.</p></details> | 28 pages, 6 figures |
| **[EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods on NLP Models](http://arxiv.org/abs/2505.01238v1)** | 2025-05-02 | <details><summary>Show</summary><p>As Natural Language Processing (NLP) models continue to evolve and become integral to high-stakes applications, ensuring their interpretability remains a critical challenge. Given the growing variety of explainability methods and diverse stakeholder requirements, frameworks that help stakeholders select appropriate explanations tailored to their specific use cases are increasingly important. To address this need, we introduce EvalxNLP, a Python framework for benchmarking state-of-the-art feature attribution methods for transformer-based NLP models. EvalxNLP integrates eight widely recognized explainability techniques from the Explainable AI (XAI) literature, enabling users to generate and evaluate explanations based on key properties such as faithfulness, plausibility, and complexity. Our framework also provides interactive, LLM-based textual explanations, facilitating user understanding of the generated explanations and evaluation outcomes. Human evaluation results indicate high user satisfaction with EvalxNLP, suggesting it is a promising framework for benchmarking explanation methods across diverse user groups. By offering a user-friendly and extensible platform, EvalxNLP aims at democratizing explainability tools and supporting the systematic comparison and advancement of XAI techniques in NLP.</p></details> | <details><summary>Accep...</summary><p>Accepted to the xAI World Conference (2025) - System Demonstration</p></details> |
| **[Robust Deep Learning-Based Physical Layer Communications: Strategies and Approaches](http://arxiv.org/abs/2505.01234v1)** | 2025-05-02 | <details><summary>Show</summary><p>Deep learning (DL) has emerged as a transformative technology with immense potential to reshape the sixth-generation (6G) wireless communication network. By utilizing advanced algorithms for feature extraction and pattern recognition, DL provides unprecedented capabilities in optimizing the network efficiency and performance, particularly in physical layer communications. Although DL technologies present the great potential, they also face significant challenges related to the robustness, which are expected to intensify in the complex and demanding 6G environment. Specifically, current DL models typically exhibit substantial performance degradation in dynamic environments with time-varying channels, interference of noise and different scenarios, which affect their effectiveness in diverse real-world applications. This paper provides a comprehensive overview of strategies and approaches for robust DL-based methods in physical layer communications. First we introduce the key challenges that current DL models face. Then we delve into a detailed examination of DL approaches specifically tailored to enhance robustness in 6G, which are classified into data-driven and model-driven strategies. Finally, we verify the effectiveness of these methods by case studies and outline future research directions.</p></details> | <details><summary>8 pag...</summary><p>8 pages, 3 figures. Accept by IEEE Network Magazine</p></details> |
| **[High Dynamic Range Novel View Synthesis with Single Exposure](http://arxiv.org/abs/2505.01212v1)** | 2025-05-02 | <details><summary>Show</summary><p>High Dynamic Range Novel View Synthesis (HDR-NVS) aims to establish a 3D scene HDR model from Low Dynamic Range (LDR) imagery. Typically, multiple-exposure LDR images are employed to capture a wider range of brightness levels in a scene, as a single LDR image cannot represent both the brightest and darkest regions simultaneously. While effective, this multiple-exposure HDR-NVS approach has significant limitations, including susceptibility to motion artifacts (e.g., ghosting and blurring), high capture and storage costs. To overcome these challenges, we introduce, for the first time, the single-exposure HDR-NVS problem, where only single exposure LDR images are available during training. We further introduce a novel approach, Mono-HDR-3D, featuring two dedicated modules formulated by the LDR image formation principles, one for converting LDR colors to HDR counterparts, and the other for transforming HDR images to LDR format so that unsupervised learning is enabled in a closed loop. Designed as a meta-algorithm, our approach can be seamlessly integrated with existing NVS models. Extensive experiments show that Mono-HDR-3D significantly outperforms previous methods. Source code will be released.</p></details> | <details><summary>It ha...</summary><p>It has been accepted by ICML 2025</p></details> |
| **[A Causal World Model Underlying Next Token Prediction: Exploring GPT in a Controlled Environment](http://arxiv.org/abs/2412.07446v3)** | 2025-05-02 | <details><summary>Show</summary><p>Do generative pre-trained transformer (GPT) models, trained only to predict the next token, implicitly learn a world model from which a sequence is generated one token at a time? We address this question by deriving a causal interpretation of the attention mechanism in GPT, and suggesting a causal world model that arises from this interpretation. Furthermore, we propose that GPT models, at inference time, can be utilized for zero-shot causal structure learning for input sequences and present a confidence score. Empirical evaluation is conducted in a controlled environment using the setup and rules of the Othello and Chess strategy games. A GPT, pre-trained on real-world games played with the intention of winning, is tested on out-of-distribution synthetic data consisting of sequences of random legal moves. We find that the GPT model is likely to generate legal next moves for out-of-distribution sequences for which a causal structure is encoded in the attention mechanism with high confidence. In cases for which the GPT model generates illegal moves it also fails to capture any causal structure.</p></details> | <details><summary>Inter...</summary><p>International Conference on Machine Learning (ICML), 2025</p></details> |
| **[Going deep and going wide: Counting logic and homomorphism indistinguishability over graphs of bounded treedepth and treewidth](http://arxiv.org/abs/2505.01193v1)** | 2025-05-02 | <details><summary>Show</summary><p>We study the expressive power of first-order logic with counting quantifiers, especially the k-variable and quantifier-rank-q fragment C^k_q, using homomorphism indistinguishability. Recently, Dawar, Jakl, and Reggio~(2021) proved that two graphs satisfy the same C^k_q-sentences if and only if they are homomorphism indistinguishable over the class T^k_q of graphs admitting a k-pebble forest cover of depth q. After reproving this result using elementary means, we provide a graph-theoretic analysis of the graph class T^k_q. This allows us to separate T^k_q from the intersection TW_{k-1} \cap TD_q, provided that q is sufficiently larger than k. Here TW_{k-1} is the class of all graphs of treewidth at most k-1 and TD_q is the class of all graphs of treedepth at most q. We are able to lift this separation to a (semantic) separation of the respective homomorphism indistinguishability relations \equiv_{T^k_q} and \equiv_{TW_{k-1} \cap TD_q}. We do this by showing that the classes TD_q and T^k_q are homomorphism distinguishing closed, as conjectured by Roberson~(2022). In order to prove Roberson's conjecture for T^k_q we characterise T^k_q in terms of a monotone Cops-and-Robber game. The crux is to prove that if Cop has a winning strategy then Cop also has a winning strategy that is monotone. To that end, we show how to transform Cops' winning strategy into a pree-tree-decomposition, which is inspired by decompositions of matroids, and then applying an intricate breadth-first `cleaning up' procedure along the pree-tree-decomposition (which may temporarily lose the property of representing a strategy), in order to achieve monotonicity while controlling the number of rounds simultaneously across all branches of the decomposition via a vertex exchange argument.</p></details> | <details><summary>This ...</summary><p>This work extends articles arXiv:2308.06044 and arXiv:2402.09139 which were presented at CSL and MFCS 2024. It is also part of second author's PhD thesis</p></details> |
| **[HarmoniCa: Harmonizing Training and Inference for Better Feature Caching in Diffusion Transformer Acceleration](http://arxiv.org/abs/2410.01723v4)** | 2025-05-02 | <details><summary>Show</summary><p>Diffusion Transformers (DiTs) excel in generative tasks but face practical deployment challenges due to high inference costs. Feature caching, which stores and retrieves redundant computations, offers the potential for acceleration. Existing learning-based caching, though adaptive, overlooks the impact of the prior timestep. It also suffers from misaligned objectives--aligned predicted noise vs. high-quality images--between training and inference. These two discrepancies compromise both performance and efficiency. To this end, we harmonize training and inference with a novel learning-based caching framework dubbed HarmoniCa. It first incorporates Step-Wise Denoising Training (SDT) to ensure the continuity of the denoising process, where prior steps can be leveraged. In addition, an Image Error Proxy-Guided Objective (IEPO) is applied to balance image quality against cache utilization through an efficient proxy to approximate the image error. Extensive experiments across $8$ models, $4$ samplers, and resolutions from $256\times256$ to $2K$ demonstrate superior performance and speedup of our framework. For instance, it achieves over $40\%$ latency reduction (i.e., $2.07\times$ theoretical speedup) and improved performance on PixArt-$\alpha$. Remarkably, our image-free approach reduces training time by $25\%$ compared with the previous method. Our code is available at https://github.com/ModelTC/HarmoniCa.</p></details> | <details><summary>Accep...</summary><p>Accepted by ICML 2025</p></details> |
| **[Continuous Aperture Array (CAPA)-Based Multi-Group Multicast Communications](http://arxiv.org/abs/2505.01190v1)** | 2025-05-02 | <details><summary>Show</summary><p>A continuous aperture array (CAPA)-based multi-group multicast communication system is investigated. An integral-based CAPA multi-group multicast beamforming design is formulated for the maximization of the system energy efficiency (EE), subject to a minimum multicast SE constraint of each user group and a total transmit power constraint. To address this non-econvex fractional programming problem, the Dinkelbach's method is employed. Within the Dinkelbach's framework, the non-convex group-wise multicast spectral efficiency (SE) constraint is first equivalently transformed into a tractable form with auxiliary variables. Then, an efficient block coordinate descent (BCD)-based algorithm is developed to solve the reformulated problem. The CAPA beamforming design subproblem can be optimally solved via the Lagrangian dual method and the calculus of variations (CoV) theory. It reveals that the optimal CAPA beamformer should be a combination of all the groups' user channels. To further reduce the computational complexity, a low-complexity zero-forcing (ZF)-based approach is proposed. The closed-form ZF CAPA beamformer is derived using each group's most representative user channel to mitigate the inter-group interference while ensuring the intra-group multicast performance. Then, the beamforming design subproblem in the BCD-based algorithm becomes a convex power allocation subproblem, which can be efficiently solved. Numerical results demonstrate that 1) the CAPA can significantly improve the EE compared to conventional spatially discrete arrays (SPDAs); 2) due to the enhanced spatial resolutions, increasing the aperture size of CAPA is not always beneficial for EE enhancement in multicast scenarios; and 3) wider user distributions of each group cause a significant EE degradation of CAPA compared to SPDA.</p></details> | 13 pages, 6 pages |
| **[Fast and Low-Cost Genomic Foundation Models via Outlier Removal](http://arxiv.org/abs/2505.00598v2)** | 2025-05-02 | <details><summary>Show</summary><p>To address the challenge of scarce computational resources in genomic modeling, we introduce GERM, a genomic foundation model with strong compression performance and fast adaptability. GERM improves upon models like DNABERT-2 by eliminating outliers that hinder low-rank adaptation and post-training quantization, enhancing both efficiency and robustness. We replace the vanilla attention layer with an outlier-free mechanism inspired by associative memory models. By removing outliers during both pre-training and fine-tuning, this approach accelerates adaptation, reduces computational costs, and enhances quantization robustness within acceptable loss margins. Additionally, we propose GERM-T, a strategy that employs small-step continual learning within the outlier-free framework, leveraging original checkpoints to avoid retraining from scratch. Empirically, GERM improves fine-tuning performance by 37.98% and quantization by 64.34% over the baseline model. It also reduces average kurtosis by 92.14% and maximum infinity norm by 82.77%. Compared to leading methods, GERM consistently delivers superior performance, offering a practical solution for genomic modeling in resource-constrained settings. Code is available at https://github.com/MAGICS-LAB/GERM.</p></details> | <details><summary>Inter...</summary><p>International Conference on Machine Learning (ICML) 2025</p></details> |
| **[Rethinking Latent Redundancy in Behavior Cloning: An Information Bottleneck Approach for Robot Manipulation](http://arxiv.org/abs/2502.02853v3)** | 2025-05-02 | <details><summary>Show</summary><p>Behavior Cloning (BC) is a widely adopted visual imitation learning method in robot manipulation. Current BC approaches often enhance generalization by leveraging large datasets and incorporating additional visual and textual modalities to capture more diverse information. However, these methods overlook whether the learned representations contain redundant information and lack a solid theoretical foundation to guide the learning process. To address these limitations, we adopt an information-theoretic perspective and introduce mutual information to quantify and mitigate redundancy in latent representations. Building on this, we incorporate the Information Bottleneck (IB) principle into BC, which extends the idea of reducing redundancy by providing a structured framework for compressing irrelevant information while preserving task-relevant features. This work presents the first comprehensive study on redundancy in latent representations across various methods, backbones, and experimental settings, while extending the generalizability of the IB to BC. Extensive experiments and analyses on the CortexBench and LIBERO benchmarks demonstrate significant performance improvements with IB, underscoring the importance of reducing input data redundancy and highlighting its practical value for more practical applications. Project Page: https://baishuanghao.github.io/BC-IB.github.io.</p></details> | <details><summary>Accep...</summary><p>Accepted by ICML 2025</p></details> |
| **[GeloVec: Higher Dimensional Geometric Smoothing for Coherent Visual Feature Extraction in Image Segmentation](http://arxiv.org/abs/2505.01057v1)** | 2025-05-02 | <details><summary>Show</summary><p>This paper introduces GeloVec, a new CNN-based attention smoothing framework for semantic segmentation that addresses critical limitations in conventional approaches. While existing attention-backed segmentation methods suffer from boundary instability and contextual discontinuities during feature mapping, our framework implements a higher-dimensional geometric smoothing method to establish a robust manifold relationships between visually coherent regions. GeloVec combines modified Chebyshev distance metrics with multispatial transformations to enhance segmentation accuracy through stabilized feature extraction. The core innovation lies in the adaptive sampling weights system that calculates geometric distances in n-dimensional feature space, achieving superior edge preservation while maintaining intra-class homogeneity. The multispatial transformation matrix incorporates tensorial projections with orthogonal basis vectors, creating more discriminative feature representations without sacrificing computational efficiency. Experimental validation across multiple benchmark datasets demonstrates significant improvements in segmentation performance, with mean Intersection over Union (mIoU) gains of 2.1%, 2.7%, and 2.4% on Caltech Birds-200, LSDSC, and FSSD datasets respectively compared to state-of-the-art methods. GeloVec's mathematical foundation in Riemannian geometry provides theoretical guarantees on segmentation stability. Importantly, our framework maintains computational efficiency through parallelized implementation of geodesic transformations and exhibits strong generalization capabilities across disciplines due to the absence of information loss during transformations.</p></details> | <details><summary>13 pa...</summary><p>13 pages, 3 figures, 3 tables</p></details> |
| **[Prompt Inversion Attack against Collaborative Inference of Large Language Models](http://arxiv.org/abs/2503.09022v3)** | 2025-05-02 | <details><summary>Show</summary><p>Large language models (LLMs) have been widely applied for their remarkable capability of content generation. However, the practical use of open-source LLMs is hindered by high resource requirements, making deployment expensive and limiting widespread development. The collaborative inference is a promising solution for this problem, in which users collaborate by each hosting a subset of layers and transmitting intermediate activation. Many companies are building collaborative inference platforms to reduce LLM serving costs, leveraging users' underutilized GPUs. Despite widespread interest in collaborative inference within academia and industry, the privacy risks associated with LLM collaborative inference have not been well studied. This is largely because of the challenge posed by inverting LLM activation due to its strong non-linearity. In this paper, to validate the severity of privacy threats in LLM collaborative inference, we introduce the concept of prompt inversion attack (PIA), where a malicious participant intends to recover the input prompt through the activation transmitted by its previous participant. Extensive experiments show that our PIA method substantially outperforms existing baselines. For example, our method achieves an 88.4\% token accuracy on the Skytrax dataset with the Llama-65B model when inverting the maximum number of transformer layers, while the best baseline method only achieves 22.8\% accuracy. The results verify the effectiveness of our PIA attack and highlights its practical threat to LLM collaborative inference systems.</p></details> | <details><summary>To ap...</summary><p>To appear at IEEE Symposium on Security and Privacy 2025</p></details> |
| **[Transforming physics-informed machine learning to convex optimization](http://arxiv.org/abs/2505.01047v1)** | 2025-05-02 | <details><summary>Show</summary><p>Physics-Informed Machine Learning (PIML) offers a powerful paradigm of integrating data with physical laws to address important scientific problems, such as parameter estimation, inferring hidden physics, equation discovery, and state prediction, etc. However, PIML still faces many serious optimization challenges that significantly restrict its applications. In this study, we propose a comprehensive framework that transforms PIML to convex optimization to overcome all these limitations, referred to as Convex-PIML. The linear combination of B-splines is utilized to approximate the data, promoting the convexity of the loss function. By replacing the non-convex components of the loss function with convex approximations, the problem is further converted into a sequence of successively refined approximated convex optimization problems. This conversion allows the use of well-established convex optimization algorithms, obtaining solutions effectively and efficiently. Furthermore, an adaptive knot optimization method based on error estimate is introduced to mitigate the spectral bias issue of PIML, further improving the performance. The proposed theoretically guaranteed framework is tested in scenarios with distinct types of physical prior. The results indicate that optimization problems are effectively solved in these scenarios, highlighting the potential of the framework for broad applications.</p></details> | 41 pages,17 figures |
| **[A Physics-Inspired Deep Learning Framework with Polar Coordinate Attention for Ptychographic Imaging](http://arxiv.org/abs/2412.06806v2)** | 2025-05-02 | <details><summary>Show</summary><p>Ptychographic imaging confronts inherent challenges in applying deep learning for phase retrieval from diffraction patterns. Conventional neural architectures, both convolutional neural networks and Transformer-based methods, are optimized for natural images with Euclidean spatial neighborhood-based inductive biases that exhibit geometric mismatch with the concentric coherent patterns characteristic of diffraction data in reciprocal space. In this paper, we present PPN, a physics-inspired deep learning network with Polar Coordinate Attention (PoCA) for ptychographic imaging, that aligns neural inductive biases with diffraction physics through a dual-branch architecture separating local feature extraction from non-local coherence modeling. It consists of a PoCA mechanism that replaces Euclidean spatial priors with physically consistent radial-angular correlations. PPN outperforms existing end-to-end models, with spectral and spatial analysis confirming its greater preservation of high-frequency details. Notably, PPN maintains robust performance compared to iterative methods even at low overlap ratios, making it well suited for high-throughput imaging in real-world acquisition scenarios for samples with consistent structural characteristics.</p></details> | 13 pages, 10 figures |
| **[3D Human Pose Estimation via Spatial Graph Order Attention and Temporal Body Aware Transformer](http://arxiv.org/abs/2505.01003v1)** | 2025-05-02 | <details><summary>Show</summary><p>Nowadays, Transformers and Graph Convolutional Networks (GCNs) are the prevailing techniques for 3D human pose estimation. However, Transformer-based methods either ignore the spatial neighborhood relationships between the joints when used for skeleton representations or disregard the local temporal patterns of the local joint movements in skeleton sequence modeling, while GCN-based methods often neglect the need for pose-specific representations. To address these problems, we propose a new method that exploits the graph modeling capability of GCN to represent each skeleton with multiple graphs of different orders, incorporated with a newly introduced Graph Order Attention module that dynamically emphasizes the most representative orders for each joint. The resulting spatial features of the sequence are further processed using a proposed temporal Body Aware Transformer that models the global body feature dependencies in the sequence with awareness of the local inter-skeleton feature dependencies of joints. Given that our 3D pose output aligns with the central 2D pose in the sequence, we improve the self-attention mechanism to be aware of the central pose while diminishing its focus gradually towards the first and the last poses. Extensive experiments on Human3.6m, MPIINF-3DHP, and HumanEva-I datasets demonstrate the effectiveness of the proposed method. Code and models are made available on Github.</p></details> | <details><summary>16 pa...</summary><p>16 pages, 9 figures, 7 tables</p></details> |
| **[Tree-Sliced Wasserstein Distance: A Geometric Perspective](http://arxiv.org/abs/2406.13725v2)** | 2025-05-02 | <details><summary>Show</summary><p>Many variants of Optimal Transport (OT) have been developed to address its heavy computation. Among them, notably, Sliced Wasserstein (SW) is widely used for application domains by projecting the OT problem onto one-dimensional lines, and leveraging the closed-form expression of the univariate OT to reduce the computational burden. However, projecting measures onto low-dimensional spaces can lead to a loss of topological information. To mitigate this issue, in this work, we propose to replace one-dimensional lines with a more intricate structure, called tree systems. This structure is metrizable by a tree metric, which yields a closed-form expression for OT problems on tree systems. We provide an extensive theoretical analysis to formally define tree systems with their topological properties, introduce the concept of splitting maps, which operate as the projection mechanism onto these structures, then finally propose a novel variant of Radon transform for tree systems and verify its injectivity. This framework leads to an efficient metric between measures, termed Tree-Sliced Wasserstein distance on Systems of Lines (TSW-SL). By conducting a variety of experiments on gradient flows, image style transfer, and generative models, we illustrate that our proposed approach performs favorably compared to SW and its variants.</p></details> | <details><summary>Accep...</summary><p>Accepted to ICML 2025</p></details> |
| **[Does Self-Attention Need Separate Weights in Transformers?](http://arxiv.org/abs/2412.00359v2)** | 2025-05-02 | <details><summary>Show</summary><p>The success of self-attention lies in its ability to capture long-range dependencies and enhance context understanding, but it is limited by its computational complexity and challenges in handling sequential data with inherent directionality. This work introduces a shared weight self-attention-based BERT model that only learns one weight matrix for (Key, Value, and Query) representations instead of three individual matrices for each of them. Our shared weight attention reduces the training parameter size by more than half and training time by around one-tenth. Furthermore, we demonstrate higher prediction accuracy on small tasks of GLUE over the BERT baseline and in particular a generalization power on noisy and out-of-domain data. Experimental results indicate that our shared self-attention method achieves a parameter size reduction of 66.53% in the attention block. In the GLUE dataset, the shared weight self-attention-based BERT model demonstrates accuracy improvements of 0.38%, 5.81%, and 1.06% over the standard, symmetric, and pairwise attention-based BERT models, respectively. The model and source code are available at Anonymous.</p></details> | Preprint paper |
| **[Empowering Agentic Video Analytics Systems with Video Language Models](http://arxiv.org/abs/2505.00254v2)** | 2025-05-02 | <details><summary>Show</summary><p>AI-driven video analytics has become increasingly pivotal across diverse domains. However, existing systems are often constrained to specific, predefined tasks, limiting their adaptability in open-ended analytical scenarios. The recent emergence of Video-Language Models (VLMs) as transformative technologies offers significant potential for enabling open-ended video understanding, reasoning, and analytics. Nevertheless, their limited context windows present challenges when processing ultra-long video content, which is prevalent in real-world applications. To address this, we introduce AVAS, a VLM-powered system designed for open-ended, advanced video analytics. AVAS incorporates two key innovations: (1) the near real-time construction of Event Knowledge Graphs (EKGs) for efficient indexing of long or continuous video streams, and (2) an agentic retrieval-generation mechanism that leverages EKGs to handle complex and diverse queries. Comprehensive evaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that AVAS achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy, respectively, significantly surpassing existing VLM and video Retrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video analytics in ultra-long and open-world video scenarios, we introduce a new benchmark, AVAS-100. This benchmark comprises 8 videos, each exceeding 10 hours in duration, along with 120 manually annotated, diverse, and complex question-answer pairs. On AVAS-100, AVAS achieves top-tier performance with an accuracy of 75.8%.</p></details> | 15 pages, AVAS |
| **[Tree-Sliced Wasserstein Distance with Nonlinear Projection](http://arxiv.org/abs/2505.00968v1)** | 2025-05-02 | <details><summary>Show</summary><p>Tree-Sliced methods have recently emerged as an alternative to the traditional Sliced Wasserstein (SW) distance, replacing one-dimensional lines with tree-based metric spaces and incorporating a splitting mechanism for projecting measures. This approach enhances the ability to capture the topological structures of integration domains in Sliced Optimal Transport while maintaining low computational costs. Building on this foundation, we propose a novel nonlinear projectional framework for the Tree-Sliced Wasserstein (TSW) distance, substituting the linear projections in earlier versions with general projections, while ensuring the injectivity of the associated Radon Transform and preserving the well-definedness of the resulting metric. By designing appropriate projections, we construct efficient metrics for measures on both Euclidean spaces and spheres. Finally, we validate our proposed metric through extensive numerical experiments for Euclidean and spherical datasets. Applications include gradient flows, self-supervised learning, and generative models, where our methods demonstrate significant improvements over recent SW and TSW variants.</p></details> | <details><summary>Accep...</summary><p>Accepted at ICML 2025</p></details> |
| **[SemSpaceFL: A Collaborative Hierarchical Federated Learning Framework for Semantic Communication in 6G LEO Satellites](http://arxiv.org/abs/2505.00966v1)** | 2025-05-02 | <details><summary>Show</summary><p>The advent of the sixth-generation (6G) wireless networks, enhanced by artificial intelligence, promises ubiquitous connectivity through Low Earth Orbit (LEO) satellites. These satellites are capable of collecting vast amounts of geographically diverse and real-time data, which can be immensely valuable for training intelligent models. However, limited inter-satellite communication and data privacy constraints hinder data collection on a single server for training. Therefore, we propose SemSpaceFL, a novel hierarchical federated learning (HFL) framework for LEO satellite networks, with integrated semantic communication capabilities. Our framework introduces a two-tier aggregation architecture where satellite models are first aggregated at regional gateways before final consolidation at a cloud server, which explicitly accounts for satellite mobility patterns and energy constraints. The key innovation lies in our novel aggregation approach, which dynamically adjusts the contribution of each satellite based on its trajectory and association with different gateways, which ensures stable model convergence despite the highly dynamic nature of LEO constellations. To further enhance communication efficiency, we incorporate semantic encoding-decoding techniques trained through the proposed HFL framework, which enables intelligent data compression while maintaining signal integrity. Our experimental results demonstrate that the proposed aggregation strategy achieves superior performance and faster convergence compared to existing benchmarks, while effectively managing the challenges of satellite mobility and energy limitations in dynamic LEO networks.</p></details> | <details><summary>13 pa...</summary><p>13 pages, 7 figures, and 5 tables</p></details> |
| **[DriveGPT: Scaling Autoregressive Behavior Models for Driving](http://arxiv.org/abs/2412.14415v3)** | 2025-05-02 | <details><summary>Show</summary><p>We present DriveGPT, a scalable behavior model for autonomous driving. We model driving as a sequential decision-making task, and learn a transformer model to predict future agent states as tokens in an autoregressive fashion. We scale up our model parameters and training data by multiple orders of magnitude, enabling us to explore the scaling properties in terms of dataset size, model parameters, and compute. We evaluate DriveGPT across different scales in a planning task, through both quantitative metrics and qualitative examples, including closed-loop driving in complex real-world scenarios. In a separate prediction task, DriveGPT outperforms state-of-the-art baselines and exhibits improved performance by pretraining on a large-scale dataset, further validating the benefits of data scaling.</p></details> | <details><summary>ICML ...</summary><p>ICML 2025. 14 pages, 17 figures, 8 tables, and 1 video link</p></details> |
| **[Enhancing Realism in Holographic Augmented Reality Displays through Occlusion Handling](http://arxiv.org/abs/2505.00942v1)** | 2025-05-02 | <details><summary>Show</summary><p>In this paper, an occlusion-capable holographic augmented-reality (AR) display is proposed, and its ability to enhance AR imagery through occlusion is demonstrated. Holographic displays can generate ideal three-dimensional (3D) virtual images and have recently shown rapid advancements, particularly in noise reduction through learning-based approaches. However, these displays still face challenges in improving image quality for AR scenarios because holographic virtual images are simply superimposed onto the real world, leading to a loss of contrast and visibility. To address this, an occlusion optics, which can mask designated areas of the real world, is incorporated into holographic AR displays. The proposed system employs a folded 4f system with a digital micromirror device and sequentially operates as both a real-world mask and an active Fourier filter. This approach transforms traditionally translucent holographic images into perceptually opaque ones while simultaneously eliminating unwanted noise terms from pixelated holographic displays. Furthermore, active Fourier filtering expands the virtual image field of view through time-multiplexed operation and supports a novel binary hologram optimization algorithm that performs especially well for sparse virtual content. The implementation successfully achieves opaque holographic 3D image presentation, significantly improving contrast and image quality while producing highly realistic 3D AR scenes with optically cast shadows.</p></details> | 24 pages, 10 figures |
| **[A Self-Supervised Transformer for Unusable Shared Bike Detection](http://arxiv.org/abs/2505.00932v1)** | 2025-05-02 | <details><summary>Show</summary><p>The rapid expansion of bike-sharing systems (BSS) has greatly improved urban "last-mile" connectivity, yet large-scale deployments face escalating operational challenges, particularly in detecting faulty bikes. Existing detection approaches either rely on static model-based thresholds that overlook dynamic spatiotemporal (ST) usage patterns or employ supervised learning methods that struggle with label scarcity and class imbalance. To address these limitations, this paper proposes a novel Self-Supervised Transformer (SSTransformer) framework for automatically detecting unusable shared bikes, leveraging ST features extracted from GPS trajectories and trip records. The model incorporates a self-supervised pre-training strategy to enhance its feature extraction capabilities, followed by fine-tuning for efficient status recognition. In the pre-training phase, the Transformer encoder learns generalized representations of bike movement via a self-supervised objective; in the fine-tuning phase, the encoder is adapted to a downstream binary classification task. Comprehensive experiments on a real-world dataset of 10,730 bikes (1,870 unusable, 8,860 normal) from Chengdu, China, demonstrate that SSTransformer significantly outperforms traditional machine learning, ensemble learning, and deep learning baselines, achieving the best accuracy (97.81%), precision (0.8889), and F1-score (0.9358). This work highlights the effectiveness of self-supervised Transformer on ST data for capturing complex anomalies in BSS, paving the way toward more reliable and scalable maintenance solutions for shared mobility.</p></details> | <details><summary>6 pag...</summary><p>6 pages, 5 figures, under review by the 2025 IEEE International Conference on Intelligent Transportation Systems (IEEE ITSC 2025)</p></details> |
| **[How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias](http://arxiv.org/abs/2505.00926v1)** | 2025-05-02 | <details><summary>Show</summary><p>Language recognition tasks are fundamental in natural language processing (NLP) and have been widely used to benchmark the performance of large language models (LLMs). These tasks also play a crucial role in explaining the working mechanisms of transformers. In this work, we focus on two representative tasks in the category of regular language recognition, known as `even pairs' and `parity check', the aim of which is to determine whether the occurrences of certain subsequences in a given sequence are even. Our goal is to explore how a one-layer transformer, consisting of an attention layer followed by a linear layer, learns to solve these tasks by theoretically analyzing its training dynamics under gradient descent. While even pairs can be solved directly by a one-layer transformer, parity check need to be solved by integrating Chain-of-Thought (CoT), either into the inference stage of a transformer well-trained for the even pairs task, or into the training of a one-layer transformer. For both problems, our analysis shows that the joint training of attention and linear layers exhibits two distinct phases. In the first phase, the attention layer grows rapidly, mapping data sequences into separable vectors. In the second phase, the attention layer becomes stable, while the linear layer grows logarithmically and approaches in direction to a max-margin hyperplane that correctly separates the attention layer outputs into positive and negative samples, and the loss decreases at a rate of $O(1/t)$. Our experiments validate those theoretical results.</p></details> | <details><summary>accep...</summary><p>accepted by ICML 2025</p></details> |
| **[Rethinking Time Encoding via Learnable Transformation Functions](http://arxiv.org/abs/2505.00887v1)** | 2025-05-01 | <details><summary>Show</summary><p>Effectively modeling time information and incorporating it into applications or models involving chronologically occurring events is crucial. Real-world scenarios often involve diverse and complex time patterns, which pose significant challenges for time encoding methods. While previous methods focus on capturing time patterns, many rely on specific inductive biases, such as using trigonometric functions to model periodicity. This narrow focus on single-pattern modeling makes them less effective in handling the diversity and complexities of real-world time patterns. In this paper, we investigate to improve the existing commonly used time encoding methods and introduce Learnable Transformation-based Generalized Time Encoding (LeTE). We propose using deep function learning techniques to parameterize non-linear transformations in time encoding, making them learnable and capable of modeling generalized time patterns, including diverse and complex temporal dynamics. By enabling learnable transformations, LeTE encompasses previous methods as specific cases and allows seamless integration into a wide range of tasks. Through extensive experiments across diverse domains, we demonstrate the versatility and effectiveness of LeTE.</p></details> | <details><summary>26 pa...</summary><p>26 pages, 13 figures, 10 tables</p></details> |

## Fast Inference
| **Title** | **Date** | **Abstract** | **Comment** |
| --- | --- | --- | --- |
| **[Bayesian Emulation of Grey-Box Multi-Model Ensembles Exploiting Known Interior Structure](http://arxiv.org/abs/2406.08367v2)** | 2025-05-02 | <details><summary>Show</summary><p>Computer models are widely used to study complex real world physical systems. However, there are major limitations to their direct use including: their complex structure; large numbers of inputs and outputs; and long evaluation times. Bayesian emulators are an effective means of addressing these challenges providing fast and efficient statistical approximation for computer model outputs. It is commonly assumed that computer models behave like a ``black-box'' function with no knowledge of the output prior to its evaluation. This ensures that emulators are generalisable but potentially limits their accuracy compared with exploiting such knowledge of constrained or structured output behaviour. We assume a ``grey-box'' computer model and develop a methodological toolkit for its analysis. This includes: multi-model ensemble subsampling to identifying a representative model subset to reduce computational expense; constructing a targeted Bayesian design for optimisation or decision support; a ``divide-and-conquer'' approach to emulating sums of outputs; structured emulators exploiting known constrained and structured behaviour of constituent outputs through splitting the parameter space and imposing truncations; emulation of sums of time series outputs; and emulation of multi-model ensemble outputs. Combining these methods establishes a hierarchical emulation framework which achieves greater physical interpretability and more accurate emulator predictions. This research is motivated by and applied to the commercially important TNO OLYMPUS Well Control Optimisation Challenge from the petroleum industry which we re-express as a decision support under uncertainty problem. We thus encourage users to examine their ``black-box'' simulators to achieve superior emulator accuracy.</p></details> | 50 pages, 15 figures |
| **[An Efficient Matrix Multiplication Algorithm for Accelerating Inference in Binary and Ternary Neural Networks](http://arxiv.org/abs/2411.06360v3)** | 2025-05-02 | <details><summary>Show</summary><p>Despite their tremendous success and versatility, Deep Neural Networks (DNNs) such as Large Language Models (LLMs) suffer from inference inefficiency and rely on advanced computational infrastructure. To address these challenges and make these models more accessible and cost-effective, in this paper, we propose algorithms to improve the inference time and memory efficiency of DNNs with binary and ternary weight matrices. Particularly focusing on matrix multiplication as the bottleneck operation of inference, we observe that, once trained, the weight matrices of a model no longer change. This allows us to preprocess these matrices and create indices that help reduce the storage requirements by a logarithmic factor while enabling our efficient inference algorithms. Specifically, for a $n\times n$ weight matrix, our efficient algorithm guarantees a time complexity of $O(\frac{n^2}{\log n})$, a logarithmic factor improvement over the standard vector-matrix multiplication. Besides theoretical analysis, we conduct extensive experiments to evaluate the practical efficiency of our algorithms. Our results confirm the superiority of our approach both with respect to time and memory, as we observed a reduction in the multiplication time up to 29x and memory usage up to 6x. When applied to LLMs, our experiments show up to a 5.24x speedup in the inference time.</p></details> | <details><summary>Accep...</summary><p>Accepted at ICML 2025</p></details> |
| **[Project-and-Fuse: Improving RGB-D Semantic Segmentation via Graph Convolution Networks](http://arxiv.org/abs/2501.18851v3)** | 2025-05-02 | <details><summary>Show</summary><p>Most existing RGB-D semantic segmentation methods focus on the feature level fusion, including complex cross-modality and cross-scale fusion modules. However, these methods may cause misalignment problem in the feature fusion process and counter-intuitive patches in the segmentation results. Inspired by the popular pixel-node-pixel pipeline, we propose to 1) fuse features from two modalities in a late fusion style, during which the geometric feature injection is guided by texture feature prior; 2) employ Graph Neural Networks (GNNs) on the fused feature to alleviate the emergence of irregular patches by inferring patch relationship. At the 3D feature extraction stage, we argue that traditional CNNs are not efficient enough for depth maps. So, we encode depth map into normal map, after which CNNs can easily extract object surface tendencies.At projection matrix generation stage, we find the existence of Biased-Assignment and Ambiguous-Locality issues in the original pipeline. Therefore, we propose to 1) adopt the Kullback-Leibler Loss to ensure no missing important pixel features, which can be viewed as hard pixel mining process; 2) connect regions that are close to each other in the Euclidean space as well as in the semantic space with larger edge weights so that location informations can been considered. Extensive experiments on two public datasets, NYU-DepthV2 and SUN RGB-D, have shown that our approach can consistently boost the performance of RGB-D semantic segmentation task.</p></details> | <details><summary>I hav...</summary><p>I have decided to withdraw this paper because I have recently obtained some new data and insights during my ongoing research</p></details> |
| **[MoDeGPT: Modular Decomposition for Large Language Model Compression](http://arxiv.org/abs/2408.09632v5)** | 2025-05-02 | <details><summary>Show</summary><p>Large Language Models (LLMs) have reshaped the landscape of artificial intelligence by demonstrating exceptional performance across various tasks. However, substantial computational requirements make their deployment challenging on devices with limited resources. Recently, compression methods using low-rank matrix techniques have shown promise, yet these often lead to degraded accuracy or introduce significant overhead in parameters and inference latency. This paper introduces \textbf{Mo}dular \textbf{De}composition (MoDeGPT), a novel structured compression framework that does not need recovery fine-tuning while resolving the above drawbacks. MoDeGPT partitions the Transformer block into modules comprised of matrix pairs and reduces the hidden dimensions via reconstructing the module-level outputs. MoDeGPT is developed based on a theoretical framework that utilizes three well-established matrix decomposition algorithms -- Nystr\"om approximation, CR decomposition, and SVD -- and applies them to our redefined transformer modules. Our comprehensive experiments show MoDeGPT, without backward propagation, matches or surpasses previous structured compression methods that rely on gradient information, and saves 98% of compute costs on compressing a 13B model. On \textsc{Llama}-2/3 and OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30% compression rates. Moreover, the compression can be done on a single GPU within a few hours and increases the inference throughput by up to 46%.</p></details> | ICLR 2025 Oral |
| **[Design-Based Inference under Random Potential Outcomes via Riesz Representation](http://arxiv.org/abs/2505.01324v1)** | 2025-05-02 | <details><summary>Show</summary><p>We introduce a general framework for design-based causal inference that accommodates stochastic potential outcomes, thereby extending the classical Neyman-Rubin setup in which outcomes are treated as fixed. In our formulation, each unit's potential outcome is modelled as a function $\tilde{y}_i(z, \omega)$, where $\omega$ denotes latent randomness external to the treatment assignment. Building on recent work that connects design-based estimation with the Riesz representation theorem, we construct causal estimators by embedding potential outcomes in a Hilbert space and defining treatment effects as linear functionals. This allows us to derive unbiased and consistent estimators, even when potential outcomes exhibit random variation. The framework retains the key advantage of design-based analysis, namely, the use of a known randomisation scheme for identification, while enabling inference in settings with inherent stochasticity. We establish large-sample properties under local dependence, provide a variance estimator compatible with sparse dependency structures, and illustrate the method through a simulation. Our results unify design-based reasoning with random-outcome modelling, broadening the applicability of causal inference in complex experimental environments.</p></details> | <details><summary>37 pa...</summary><p>37 pages, 2 figures, 2 Tables, 2 Algorithms. Preprint prepared for journal submission</p></details> |
| **[Towards One Model for Classical Dimensionality Reduction: A Probabilistic Perspective on UMAP and t-SNE](http://arxiv.org/abs/2405.17412v4)** | 2025-05-02 | <details><summary>Show</summary><p>This paper shows that dimensionality reduction methods such as UMAP and t-SNE, can be approximately recast as MAP inference methods corresponding to a model introduced in Ravuri et al. (2023), that describes the graph Laplacian (an estimate of the data precision matrix) using a Wishart distribution, with a mean given by a non-linear covariance function evaluated on the latents. This interpretation offers deeper theoretical and semantic insights into such algorithms, and forging a connection to Gaussian process latent variable models by showing that well-known kernels can be used to describe covariances implied by graph Laplacians. We also introduce tools with which similar dimensionality reduction methods can be studied.</p></details> | AABI version |
| **[Improving Continual Learning Performance and Efficiency with Auxiliary Classifiers](http://arxiv.org/abs/2403.07404v3)** | 2025-05-02 | <details><summary>Show</summary><p>Continual learning is crucial for applying machine learning in challenging, dynamic, and often resource-constrained environments. However, catastrophic forgetting - overwriting previously learned knowledge when new information is acquired - remains a major challenge. In this work, we examine the intermediate representations in neural network layers during continual learning and find that such representations are less prone to forgetting, highlighting their potential to accelerate computation. Motivated by these findings, we propose to use auxiliary classifiers(ACs) to enhance performance and demonstrate that integrating ACs into various continual learning methods consistently improves accuracy across diverse evaluation settings, yielding an average 10% relative gain. We also leverage the ACs to reduce the average cost of the inference by 10-60% without compromising accuracy, enabling the model to return the predictions before computing all the layers. Our approach provides a scalable and efficient solution for continual learning.</p></details> | ICML 2025 |
| **[MultiGran-STGCNFog: Towards Accurate and High-Throughput Inference for Multi-Granular Spatiotemporal Traffic Forecasting](http://arxiv.org/abs/2505.01279v1)** | 2025-05-02 | <details><summary>Show</summary><p>Accurate traffic forecasting and swift inference provision are essential for intelligent transportation systems. However, the present Graph Convolutional Network (GCN)-based approaches cannot extract and fuse multi-granular spatiotemporal features across various spatial and temporal scales sufficiently, proven to yield less accurate forecasts. Besides, additional feature extraction branches introduced in prior studies critically increased model complexity and extended inference time, making it challenging to provide fast inference for traffic forecasting. In this paper, we propose MultiGran-STGCNFog, an efficient fog distributed inference system with a novel traffic forecasting model that employs multi-granular spatiotemporal feature fusion on generated dynamic traffic graphs to fully capture interdependent traffic dynamics. The proposed scheduling algorithm GA-DPHDS, optimizing layer execution order and layer-device scheduling scheme simultaneously, contributes to considerable inference throughput improvement by leveraging heterogeneous fog devices in a pipelined manner. Extensive experiments on real-world datasets demonstrate the superiority of the proposed method over selected baselines.</p></details> | <details><summary>nine ...</summary><p>nine pages and five figures included</p></details> |
| **[Doubly-Robust Functional Average Treatment Effect Estimation](http://arxiv.org/abs/2501.06024v2)** | 2025-05-02 | <details><summary>Show</summary><p>Understanding causal relationships in the presence of complex, structured data remains a central challenge in modern statistics and science in general. While traditional causal inference methods are well-suited for scalar outcomes, many scientific applications demand tools capable of handling functional data -- outcomes observed as functions over continuous domains such as time or space. Motivated by this need, we propose DR-FoS, a novel method for estimating the Functional Average Treatment Effect (FATE) in observational studies with functional outcomes. DR-FoS exhibits double robustness properties, ensuring consistent estimation of FATE even if either the outcome or the treatment assignment model is misspecified. By leveraging recent advances in functional data analysis and causal inference, we establish the asymptotic properties of the estimator, proving its convergence to a Gaussian process. This guarantees valid inference with simultaneous confidence bands across the entire functional domain. Through extensive simulations, we show that DR-FoS achieves robust performance under a wide range of model specifications. Finally, we illustrate the utility of DR-FoS in a real-world application, analyzing functional outcomes to uncover meaningful causal insights in the SHARE (Survey of Health, Aging and Retirement in Europe) dataset.</p></details> | 22 pages, 5 figures |
| **[Fusing Foveal Fixations Using Linear Retinal Transformations and Bayesian Experimental Design](http://arxiv.org/abs/2505.01249v1)** | 2025-05-02 | <details><summary>Show</summary><p>Humans (and many vertebrates) face the problem of fusing together multiple fixations of a scene in order to obtain a representation of the whole, where each fixation uses a high-resolution fovea and decreasing resolution in the periphery. In this paper we explicitly represent the retinal transformation of a fixation as a linear downsampling of a high-resolution latent image of the scene, exploiting the known geometry. This linear transformation allows us to carry out exact inference for the latent variables in factor analysis (FA) and mixtures of FA models of the scene. Further, this allows us to formulate and solve the choice of "where to look next" as a Bayesian experimental design problem using the Expected Information Gain criterion. Experiments on the Frey faces and MNIST datasets demonstrate the effectiveness of our models.</p></details> | 19 pages, 4 figures |
| **[Cyber Food Swamps: Investigating the Impacts of Online-to-Offline Food Delivery Platforms on Healthy Food Choices](http://arxiv.org/abs/2409.16601v3)** | 2025-05-02 | <details><summary>Show</summary><p>Online-to-offline (O2O) food delivery platforms have greatly expanded urban residents' access to a wide range of food options by allowing convenient ordering from distant food outlets. However, concerns persist regarding the nutritional quality of delivered food, particularly as the impact of O2O food delivery platforms on users' healthy food remains unclear. This study leverages large-scale empirical data from a leading O2O delivery platform to comprehensively analyze online food choice behaviors and how they are influenced by the online exposure to fast food restaurants, i.e., online food environment. Our analyses reveal significant variations in food preferences across demographic groups and city sizes, where male, low-income, and younger users are more likely to order fast food via O2O platforms. Besides, we also perform a comparative analysis on the food exposure differences in offline and online environments, confirming that the extended service ranges of O2O platforms can create larger "cyber food swamps". Furthermore, regression analysis highlights that a higher ratio of fast food orders is associated with "cyber food swamps", areas characterized by a higher proportion of accessible fast food restaurants. A 10% increase in this proportion raises the probability of ordering fast food by 22.0%. Moreover, a quasi-natural experiment substantiates the long-term causal effect of online food environment changes on healthy food choices. These findings underscore the need for O2O food delivery platforms to address the health implications of online food choice exposure, offering critical insights for stakeholders aiming to improve dietary health among urban populations.</p></details> | <details><summary>Confe...</summary><p>Conference Paper at ICWSM 2025</p></details> |
| **[Quantitative Attractor Analysis of High-Capacity Kernel Logistic Regression Hopfield Networks](http://arxiv.org/abs/2505.01218v1)** | 2025-05-02 | <details><summary>Show</summary><p>Traditional Hopfield networks, using Hebbian learning, face severe storage capacity limits ($\approx 0.14$ P/N) and spurious attractors. Kernel Logistic Regression (KLR) offers a non-linear approach, mapping patterns to high-dimensional feature spaces for improved separability. Our previous work showed KLR dramatically improves capacity and noise robustness over conventional methods. This paper quantitatively analyzes the attractor structures in KLR-trained networks via extensive simulations. We evaluated recall from diverse initial states across wide storage loads (up to 4.0 P/N) and noise levels. We quantified convergence rates and speed. Our analysis confirms KLR's superior performance: high capacity (up to 4.0 P/N) and robustness. The attractor landscape is remarkably "clean," with near-zero spurious fixed points. Recall failures under high load/noise are primarily due to convergence to other learned patterns, not spurious ones. Dynamics are exceptionally fast (typically 1-2 steps for high-similarity states). This characterization reveals how KLR reshapes dynamics for high-capacity associative memory, highlighting its effectiveness and contributing to AM understanding.</p></details> | 8 pages, 7 figures |
| **[A Space-Time Trade-off for Fast Self-Stabilizing Leader Election in Population Protocols](http://arxiv.org/abs/2505.01210v1)** | 2025-05-02 | <details><summary>Show</summary><p>We consider the problem of self-stabilizing leader election in the population model by Angluin, Aspnes, Diamadi, Fischer, and Peralta (JDistComp '06). The population model is a well-established and powerful model for asynchronous, distributed computation with a large number of applications. For self-stabilizing leader election, the population of $n$ anonymous agents, interacting in uniformly random pairs, must stabilize with a single leader from any possible initial configuration. The focus of this paper is to develop time-efficient self-stabilizing protocols whilst minimizing the number of states. We present a parametrized protocol, which, for a suitable setting, achieves the asymptotically optimal time $O(\log n)$ using $2^{O(n^2\log n)}$ states (throughout the paper, ``time'' refers to ``parallel time'', i.e., the number of pairwise interactions divided by $n$). This is a significant improvement over the previously best protocol Sublinear-Time-SSR due to Burman, Chen, Chen, Doty, Nowak, Severson, and Xu (PODC '21), which requires $2^{O(n^{\log n}\log n)}$ states for the same time bound. In general, for $1\le r\le n/2$, our protocol requires $2^{O(r^2\log{n})}$ states and stabilizes in time $O((n\log{n})/r)$, w.h.p.; the above result is achieved for $r=\Theta(n)$. For $r=\log^2n$ our protocol requires only sub-linear time using only $2^{O(\log^3 n)}$ states, resolving an open problem stated in that paper. Sublinear-Time-SSR requires $O(\log n\cdot n^{1/(H+1)})$ time using $2^{\Theta(n^H) \cdot \log n}$ states for all $1\le H\le\Theta(\log n)$. Similar to previous works, it solves leader election by assigning a unique rank from $1$ through $n$ to each agent. The principal bottleneck for self-stabilizing ranking usually is to detect if there exist agents with the same rank. One of our main conceptual contributions is a novel technique for collision detection.</p></details> | <details><summary>To be...</summary><p>To be published at PODC'25</p></details> |
| **[A Causal World Model Underlying Next Token Prediction: Exploring GPT in a Controlled Environment](http://arxiv.org/abs/2412.07446v3)** | 2025-05-02 | <details><summary>Show</summary><p>Do generative pre-trained transformer (GPT) models, trained only to predict the next token, implicitly learn a world model from which a sequence is generated one token at a time? We address this question by deriving a causal interpretation of the attention mechanism in GPT, and suggesting a causal world model that arises from this interpretation. Furthermore, we propose that GPT models, at inference time, can be utilized for zero-shot causal structure learning for input sequences and present a confidence score. Empirical evaluation is conducted in a controlled environment using the setup and rules of the Othello and Chess strategy games. A GPT, pre-trained on real-world games played with the intention of winning, is tested on out-of-distribution synthetic data consisting of sequences of random legal moves. We find that the GPT model is likely to generate legal next moves for out-of-distribution sequences for which a causal structure is encoded in the attention mechanism with high confidence. In cases for which the GPT model generates illegal moves it also fails to capture any causal structure.</p></details> | <details><summary>Inter...</summary><p>International Conference on Machine Learning (ICML), 2025</p></details> |
| **[HarmoniCa: Harmonizing Training and Inference for Better Feature Caching in Diffusion Transformer Acceleration](http://arxiv.org/abs/2410.01723v4)** | 2025-05-02 | <details><summary>Show</summary><p>Diffusion Transformers (DiTs) excel in generative tasks but face practical deployment challenges due to high inference costs. Feature caching, which stores and retrieves redundant computations, offers the potential for acceleration. Existing learning-based caching, though adaptive, overlooks the impact of the prior timestep. It also suffers from misaligned objectives--aligned predicted noise vs. high-quality images--between training and inference. These two discrepancies compromise both performance and efficiency. To this end, we harmonize training and inference with a novel learning-based caching framework dubbed HarmoniCa. It first incorporates Step-Wise Denoising Training (SDT) to ensure the continuity of the denoising process, where prior steps can be leveraged. In addition, an Image Error Proxy-Guided Objective (IEPO) is applied to balance image quality against cache utilization through an efficient proxy to approximate the image error. Extensive experiments across $8$ models, $4$ samplers, and resolutions from $256\times256$ to $2K$ demonstrate superior performance and speedup of our framework. For instance, it achieves over $40\%$ latency reduction (i.e., $2.07\times$ theoretical speedup) and improved performance on PixArt-$\alpha$. Remarkably, our image-free approach reduces training time by $25\%$ compared with the previous method. Our code is available at https://github.com/ModelTC/HarmoniCa.</p></details> | <details><summary>Accep...</summary><p>Accepted by ICML 2025</p></details> |
| **[TActiLE: Tiny Active LEarning for wearable devices](http://arxiv.org/abs/2505.01160v1)** | 2025-05-02 | <details><summary>Show</summary><p>Tiny Machine Learning (TinyML) algorithms have seen extensive use in recent years, enabling wearable devices to be not only connected but also genuinely intelligent by running machine learning (ML) computations directly on-device. Among such devices, smart glasses have particularly benefited from TinyML advancements. TinyML facilitates the on-device execution of the inference phase of ML algorithms on embedded and wearable devices, and more recently, it has expanded into On-device Learning (ODL), which allows both inference and learning phases to occur directly on the device. The application of ODL techniques to wearable devices is particularly compelling, as it enables the development of more personalized models that adapt based on the data of the user. However, one of the major challenges of ODL algorithms is the scarcity of labeled data collected on-device. In smart wearable contexts, requiring users to manually label large amounts of data is often impractical and could lead to user disengagement with the technology. To address this issue, this paper explores the application of Active Learning (AL) techniques, i.e., techniques that aim at minimizing the labeling effort, by actively selecting from a large quantity of unlabeled data only a small subset to be labeled and added to the training set of the algorithm. In particular, we propose TActiLE, a novel AL algorithm that selects from the stream of on-device sensor data the ones that would help the ML algorithm improve the most once coupled with labels provided by the user. TActiLE is the first Active Learning technique specifically designed for the TinyML context. We evaluate its effectiveness and efficiency through experiments on multiple image classification datasets. The results demonstrate its suitability for tiny and wearable devices.</p></details> | <details><summary>Accep...</summary><p>Accepted to the "Eyes Of The Future: Integrating Artificial Intelligence in Smart Eyewear (IAISE)" Workshop, Held at the "International Joint Conference on Neural Networks (IJCNN) 2025"</p></details> |
| **[Fast and Low-Cost Genomic Foundation Models via Outlier Removal](http://arxiv.org/abs/2505.00598v2)** | 2025-05-02 | <details><summary>Show</summary><p>To address the challenge of scarce computational resources in genomic modeling, we introduce GERM, a genomic foundation model with strong compression performance and fast adaptability. GERM improves upon models like DNABERT-2 by eliminating outliers that hinder low-rank adaptation and post-training quantization, enhancing both efficiency and robustness. We replace the vanilla attention layer with an outlier-free mechanism inspired by associative memory models. By removing outliers during both pre-training and fine-tuning, this approach accelerates adaptation, reduces computational costs, and enhances quantization robustness within acceptable loss margins. Additionally, we propose GERM-T, a strategy that employs small-step continual learning within the outlier-free framework, leveraging original checkpoints to avoid retraining from scratch. Empirically, GERM improves fine-tuning performance by 37.98% and quantization by 64.34% over the baseline model. It also reduces average kurtosis by 92.14% and maximum infinity norm by 82.77%. Compared to leading methods, GERM consistently delivers superior performance, offering a practical solution for genomic modeling in resource-constrained settings. Code is available at https://github.com/MAGICS-LAB/GERM.</p></details> | <details><summary>Inter...</summary><p>International Conference on Machine Learning (ICML) 2025</p></details> |
| **[Efficient Vocabulary-Free Fine-Grained Visual Recognition in the Age of Multimodal LLMs](http://arxiv.org/abs/2505.01064v1)** | 2025-05-02 | <details><summary>Show</summary><p>Fine-grained Visual Recognition (FGVR) involves distinguishing between visually similar categories, which is inherently challenging due to subtle inter-class differences and the need for large, expert-annotated datasets. In domains like medical imaging, such curated datasets are unavailable due to issues like privacy concerns and high annotation costs. In such scenarios lacking labeled data, an FGVR model cannot rely on a predefined set of training labels, and hence has an unconstrained output space for predictions. We refer to this task as Vocabulary-Free FGVR (VF-FGVR), where a model must predict labels from an unconstrained output space without prior label information. While recent Multimodal Large Language Models (MLLMs) show potential for VF-FGVR, querying these models for each test input is impractical because of high costs and prohibitive inference times. To address these limitations, we introduce \textbf{Nea}rest-Neighbor Label \textbf{R}efinement (NeaR), a novel approach that fine-tunes a downstream CLIP model using labels generated by an MLLM. Our approach constructs a weakly supervised dataset from a small, unlabeled training set, leveraging MLLMs for label generation. NeaR is designed to handle the noise, stochasticity, and open-endedness inherent in labels generated by MLLMs, and establishes a new benchmark for efficient VF-FGVR.</p></details> | <details><summary>prepr...</summary><p>preprint; earlier version accepted at NeurIPS 2024 Workshop on Adaptive Foundation Models</p></details> |
| **[Multi-Step Consistency Models: Fast Generation with Theoretical Guarantees](http://arxiv.org/abs/2505.01049v1)** | 2025-05-02 | <details><summary>Show</summary><p>Consistency models have recently emerged as a compelling alternative to traditional SDE based diffusion models, offering a significant acceleration in generation by producing high quality samples in very few steps. Despite their empirical success, a proper theoretic justification for their speed up is still lacking. In this work, we provide the analysis which bridges this gap, showing that given a consistency model which can map the input at a given time to arbitrary timestamps along the reverse trajectory, one can achieve KL divergence of order $ O(\varepsilon^2) $ using only $ O\left(\log\left(\frac{d}{\varepsilon}\right)\right) $ iterations with constant step size, where d is the data dimension. Additionally, under minimal assumptions on the data distribution an increasingly common setting in recent diffusion model analyses we show that a similar KL convergence guarantee can be obtained, with the number of steps scaling as $ O\left(d \log\left(\frac{d}{\varepsilon}\right)\right) $. Going further, we also provide a theoretical analysis for estimation of such consistency models, concluding that accurate learning is feasible using small discretization steps, both in smooth and non smooth settings. Notably, our results for the non smooth case yield best in class convergence rates compared to existing SDE or ODE based analyses under minimal assumptions.</p></details> | 29 pages |
| **[Prompt Inversion Attack against Collaborative Inference of Large Language Models](http://arxiv.org/abs/2503.09022v3)** | 2025-05-02 | <details><summary>Show</summary><p>Large language models (LLMs) have been widely applied for their remarkable capability of content generation. However, the practical use of open-source LLMs is hindered by high resource requirements, making deployment expensive and limiting widespread development. The collaborative inference is a promising solution for this problem, in which users collaborate by each hosting a subset of layers and transmitting intermediate activation. Many companies are building collaborative inference platforms to reduce LLM serving costs, leveraging users' underutilized GPUs. Despite widespread interest in collaborative inference within academia and industry, the privacy risks associated with LLM collaborative inference have not been well studied. This is largely because of the challenge posed by inverting LLM activation due to its strong non-linearity. In this paper, to validate the severity of privacy threats in LLM collaborative inference, we introduce the concept of prompt inversion attack (PIA), where a malicious participant intends to recover the input prompt through the activation transmitted by its previous participant. Extensive experiments show that our PIA method substantially outperforms existing baselines. For example, our method achieves an 88.4\% token accuracy on the Skytrax dataset with the Llama-65B model when inverting the maximum number of transformer layers, while the best baseline method only achieves 22.8\% accuracy. The results verify the effectiveness of our PIA attack and highlights its practical threat to LLM collaborative inference systems.</p></details> | <details><summary>To ap...</summary><p>To appear at IEEE Symposium on Security and Privacy 2025</p></details> |
| **[Transforming physics-informed machine learning to convex optimization](http://arxiv.org/abs/2505.01047v1)** | 2025-05-02 | <details><summary>Show</summary><p>Physics-Informed Machine Learning (PIML) offers a powerful paradigm of integrating data with physical laws to address important scientific problems, such as parameter estimation, inferring hidden physics, equation discovery, and state prediction, etc. However, PIML still faces many serious optimization challenges that significantly restrict its applications. In this study, we propose a comprehensive framework that transforms PIML to convex optimization to overcome all these limitations, referred to as Convex-PIML. The linear combination of B-splines is utilized to approximate the data, promoting the convexity of the loss function. By replacing the non-convex components of the loss function with convex approximations, the problem is further converted into a sequence of successively refined approximated convex optimization problems. This conversion allows the use of well-established convex optimization algorithms, obtaining solutions effectively and efficiently. Furthermore, an adaptive knot optimization method based on error estimate is introduced to mitigate the spectral bias issue of PIML, further improving the performance. The proposed theoretically guaranteed framework is tested in scenarios with distinct types of physical prior. The results indicate that optimization problems are effectively solved in these scenarios, highlighting the potential of the framework for broad applications.</p></details> | 41 pages,17 figures |
| **[ICLR: In-Context Learning of Representations](http://arxiv.org/abs/2501.00070v2)** | 2025-05-02 | <details><summary>Show</summary><p>Recent work has demonstrated that semantics specified by pretraining data influence how representations of different concepts are organized in a large language model (LLM). However, given the open-ended nature of LLMs, e.g., their ability to in-context learn, we can ask whether models alter these pretraining semantics to adopt alternative, context-specified ones. Specifically, if we provide in-context exemplars wherein a concept plays a different role than what the pretraining data suggests, do models reorganize their representations in accordance with these novel semantics? To answer this question, we take inspiration from the theory of conceptual role semantics and define a toy "graph tracing" task wherein the nodes of the graph are referenced via concepts seen during training (e.g., apple, bird, etc.) and the connectivity of the graph is defined via some predefined structure (e.g., a square grid). Given exemplars that indicate traces of random walks on the graph, we analyze intermediate representations of the model and find that as the amount of context is scaled, there is a sudden re-organization from pretrained semantic representations to in-context representations aligned with the graph structure. Further, we find that when reference concepts have correlations in their semantics (e.g., Monday, Tuesday, etc.), the context-specified graph structure is still present in the representations, but is unable to dominate the pretrained structure. To explain these results, we analogize our task to energy minimization for a predefined graph topology, providing evidence towards an implicit optimization process to infer context-specified semantics. Overall, our findings indicate scaling context-size can flexibly re-organize model representations, possibly unlocking novel capabilities.</p></details> | ICLR 2025 |
| **[Competition Dynamics Shape Algorithmic Phases of In-Context Learning](http://arxiv.org/abs/2412.01003v4)** | 2025-05-02 | <details><summary>Show</summary><p>In-Context Learning (ICL) has significantly expanded the general-purpose nature of large language models, allowing them to adapt to novel tasks using merely the inputted context. This has motivated a series of papers that analyze tractable synthetic domains and postulate precise mechanisms that may underlie ICL. However, the use of relatively distinct setups that often lack a sequence modeling nature to them makes it unclear how general the reported insights from such studies are. Motivated by this, we propose a synthetic sequence modeling task that involves learning to simulate a finite mixture of Markov chains. As we show, models trained on this task reproduce most well-known results on ICL, hence offering a unified setting for studying the concept. Building on this setup, we demonstrate we can explain a model's behavior by decomposing it into four broad algorithms that combine a fuzzy retrieval vs. inference approach with either unigram or bigram statistics of the context. These algorithms engage in a competition dynamics to dominate model behavior, with the precise experimental conditions dictating which algorithm ends up superseding others: e.g., we find merely varying context size or amount of training yields (at times sharp) transitions between which algorithm dictates the model behavior, revealing a mechanism that explains the transient nature of ICL. In this sense, we argue ICL is best thought of as a mixture of different algorithms, each with its own peculiarities, instead of a monolithic capability. This also implies that making general claims about ICL that hold universally across all settings may be infeasible.</p></details> | ICLR 2025 Spotlight |
| **[Stacked Intelligent Metasurfaces for Wireless Communications: Applications and Challenges](http://arxiv.org/abs/2407.03566v2)** | 2025-05-02 | <details><summary>Show</summary><p>The rapid growth of wireless communications has created a significant demand for high throughput, seamless connectivity, and extremely low latency. To meet these goals, a novel technology -- stacked intelligent metasurfaces (SIMs) -- has been developed to perform signal processing by directly utilizing electromagnetic waves, thus achieving incredibly fast computing speed while reducing hardware requirements. In this article, we provide an overview of SIM technology, including its underlying hardware, benefits, and exciting applications in wireless communications. Specifically, we examine the utilization of SIMs in realizing transmit beamforming and semantic encoding in the wave domain. Additionally, channel estimation in SIM-aided communication systems is discussed. Finally, we highlight potential research opportunities and identify key challenges for deploying SIMs in wireless networks to motivate future research.</p></details> | <details><summary>9 pag...</summary><p>9 pages, 4 figures, 2 tables, accepted by IEEE Wireless Communications</p></details> |
| **[Robust Root Cause Diagnosis using In-Distribution Interventions](http://arxiv.org/abs/2505.00930v1)** | 2025-05-02 | <details><summary>Show</summary><p>Diagnosing the root cause of an anomaly in a complex interconnected system is a pressing problem in today's cloud services and industrial operations. We propose In-Distribution Interventions (IDI), a novel algorithm that predicts root cause as nodes that meet two criteria: 1) **Anomaly:** root cause nodes should take on anomalous values; 2) **Fix:** had the root cause nodes assumed usual values, the target node would not have been anomalous. Prior methods of assessing the fix condition rely on counterfactuals inferred from a Structural Causal Model (SCM) trained on historical data. But since anomalies are rare and fall outside the training distribution, the fitted SCMs yield unreliable counterfactual estimates. IDI overcomes this by relying on interventional estimates obtained by solely probing the fitted SCM at in-distribution inputs. We present a theoretical analysis comparing and bounding the errors in assessing the fix condition using interventional and counterfactual estimates. We then conduct experiments by systematically varying the SCM's complexity to demonstrate the cases where IDI's interventional approach outperforms the counterfactual approach and vice versa. Experiments on both synthetic and PetShop RCD benchmark datasets demonstrate that \our\ consistently identifies true root causes more accurately and robustly than nine existing state-of-the-art RCD baselines. Code is released at https://github.com/nlokeshiisc/IDI_release.</p></details> | Accepted at ICLR-25 |
| **[How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias](http://arxiv.org/abs/2505.00926v1)** | 2025-05-02 | <details><summary>Show</summary><p>Language recognition tasks are fundamental in natural language processing (NLP) and have been widely used to benchmark the performance of large language models (LLMs). These tasks also play a crucial role in explaining the working mechanisms of transformers. In this work, we focus on two representative tasks in the category of regular language recognition, known as `even pairs' and `parity check', the aim of which is to determine whether the occurrences of certain subsequences in a given sequence are even. Our goal is to explore how a one-layer transformer, consisting of an attention layer followed by a linear layer, learns to solve these tasks by theoretically analyzing its training dynamics under gradient descent. While even pairs can be solved directly by a one-layer transformer, parity check need to be solved by integrating Chain-of-Thought (CoT), either into the inference stage of a transformer well-trained for the even pairs task, or into the training of a one-layer transformer. For both problems, our analysis shows that the joint training of attention and linear layers exhibits two distinct phases. In the first phase, the attention layer grows rapidly, mapping data sequences into separable vectors. In the second phase, the attention layer becomes stable, while the linear layer grows logarithmically and approaches in direction to a max-margin hyperplane that correctly separates the attention layer outputs into positive and negative samples, and the loss decreases at a rate of $O(1/t)$. Our experiments validate those theoretical results.</p></details> | <details><summary>accep...</summary><p>accepted by ICML 2025</p></details> |
| **[RestoreGrad: Signal Restoration Using Conditional Denoising Diffusion Models with Jointly Learned Prior](http://arxiv.org/abs/2502.13574v2)** | 2025-05-01 | <details><summary>Show</summary><p>Denoising diffusion probabilistic models (DDPMs) can be utilized for recovering a clean signal from its degraded observation(s) by conditioning the model on the degraded signal. The degraded signals are themselves contaminated versions of the clean signals; due to this correlation, they may encompass certain useful information about the target clean data distribution. However, existing adoption of the standard Gaussian as the prior distribution in turn discards such information, resulting in sub-optimal performance. In this paper, we propose to improve conditional DDPMs for signal restoration by leveraging a more informative prior that is jointly learned with the diffusion model. The proposed framework, called RestoreGrad, seamlessly integrates DDPMs into the variational autoencoder framework and exploits the correlation between the degraded and clean signals to encode a better diffusion prior. On speech and image restoration tasks, we show that RestoreGrad demonstrates faster convergence (5-10 times fewer training steps) to achieve better quality of restored signals over existing DDPM baselines, and improved robustness to using fewer sampling steps in inference time (2-2.5 times fewer), advocating the advantages of leveraging jointly learned prior for efficiency improvements in the diffusion process.</p></details> | <details><summary>Accep...</summary><p>Accepted by ICML 2025</p></details> |
| **[An Adaptive Method for Weak Supervision with Drifting Data](http://arxiv.org/abs/2306.01658v2)** | 2025-05-01 | <details><summary>Show</summary><p>We introduce an adaptive method with formal quality guarantees for weak supervision in a non-stationary setting. Our goal is to infer the unknown labels of a sequence of data by using weak supervision sources that provide independent noisy signals of the correct classification for each data point. This setting includes crowdsourcing and programmatic weak supervision. We focus on the non-stationary case, where the accuracy of the weak supervision sources can drift over time, e.g., because of changes in the underlying data distribution. Due to the drift, older data could provide misleading information to infer the label of the current data point. Previous work relied on a priori assumptions on the magnitude of the drift to decide how much data to use from the past. In contrast, our algorithm does not require any assumptions on the drift, and it adapts based on the input by dynamically varying its window size. In particular, at each step, our algorithm estimates the current accuracies of the weak supervision sources by identifying a window of past observations that guarantees a near-optimal minimization of the trade-off between the error due to the variance of the estimation and the error due to the drift. Experiments on synthetic and real-world labelers show that our approach adapts to the drift.</p></details> | <details><summary>confe...</summary><p>conference version updated</p></details> |
| **[Learning Transparent Reward Models via Unsupervised Feature Selection](http://arxiv.org/abs/2410.18608v2)** | 2025-05-01 | <details><summary>Show</summary><p>In complex real-world tasks such as robotic manipulation and autonomous driving, collecting expert demonstrations is often more straightforward than specifying precise learning objectives and task descriptions. Learning from expert data can be achieved through behavioral cloning or by learning a reward function, i.e., inverse reinforcement learning. The latter allows for training with additional data outside the training distribution, guided by the inferred reward function. We propose a novel approach to construct compact and transparent reward models from automatically selected state features. These inferred rewards have an explicit form and enable the learning of policies that closely match expert behavior by training standard reinforcement learning algorithms from scratch. We validate our method's performance in various robotic environments with continuous and high-dimensional state spaces. Webpage: \url{https://sites.google.com/view/transparent-reward}.</p></details> | 8 pages, 4 figures |
| **[NeMo-Inspector: A Visualization Tool for LLM Generation Analysis](http://arxiv.org/abs/2505.00903v1)** | 2025-05-01 | <details><summary>Show</summary><p>Adapting Large Language Models (LLMs) to novel tasks and enhancing their overall capabilities often requires large, high-quality training datasets. Synthetic data, generated at scale, serves a valuable alternative when real-world data is scarce or difficult to obtain. However, ensuring the quality of synthetic datasets is challenging, as developers must manually inspect and refine numerous samples to identify errors and areas for improvement. This process is time-consuming and requires specialized tools. We introduce NeMo-Inspector, an open-source tool designed to simplify the analysis of synthetic datasets with integrated inference capabilities. We demonstrate its effectiveness through two real-world cases. Analysis and cleaning of the synthetically generated GSM-Plus dataset with NeMo-Inspector led to a significant decrease in low-quality samples from 46.99% to 19.51%. The tool also helped identify and correct generation errors in OpenMath models, improving accuracy by 1.92% on the MATH dataset and by 4.17% on the GSM8K dataset for a Meta-Llama-3-8B model fine-tuned on synthetic data generated from Nemotron-4-340B.</p></details> | <details><summary>Prese...</summary><p>Presented at the NAACL 2025 conference</p></details> |
| **[Closed-Loop Long-Horizon Robotic Planning via Equilibrium Sequence Modeling](http://arxiv.org/abs/2410.01440v5)** | 2025-05-01 | <details><summary>Show</summary><p>In the endeavor to make autonomous robots take actions, task planning is a major challenge that requires translating high-level task descriptions to long-horizon action sequences. Despite recent advances in language model agents, they remain prone to planning errors and limited in their ability to plan ahead. To address these limitations in robotic planning, we advocate a self-refining scheme that iteratively refines a draft plan until an equilibrium is reached. Remarkably, this process can be optimized end-to-end from an analytical perspective without the need to curate additional verifiers or reward models, allowing us to train self-refining planners in a simple supervised learning fashion. Meanwhile, a nested equilibrium sequence modeling procedure is devised for efficient closed-loop planning that incorporates useful feedback from the environment (or an internal world model). Our method is evaluated on the VirtualHome-Env benchmark, showing advanced performance with improved scaling w.r.t. inference-time computation. Code is available at https://github.com/Singularity0104/equilibrium-planner.</p></details> | ICML 2025 |
| **[ArticuBot: Learning Universal Articulated Object Manipulation Policy via Large Scale Simulation](http://arxiv.org/abs/2503.03045v2)** | 2025-05-01 | <details><summary>Show</summary><p>This paper presents ArticuBot, in which a single learned policy enables a robotics system to open diverse categories of unseen articulated objects in the real world. This task has long been challenging for robotics due to the large variations in the geometry, size, and articulation types of such objects. Our system, Articubot, consists of three parts: generating a large number of demonstrations in physics-based simulation, distilling all generated demonstrations into a point cloud-based neural policy via imitation learning, and performing zero-shot sim2real transfer to real robotics systems. Utilizing sampling-based grasping and motion planning, our demonstration generalization pipeline is fast and effective, generating a total of 42.3k demonstrations over 322 training articulated objects. For policy learning, we propose a novel hierarchical policy representation, in which the high-level policy learns the sub-goal for the end-effector, and the low-level policy learns how to move the end-effector conditioned on the predicted goal. We demonstrate that this hierarchical approach achieves much better object-level generalization compared to the non-hierarchical version. We further propose a novel weighted displacement model for the high-level policy that grounds the prediction into the existing 3D structure of the scene, outperforming alternative policy representations. We show that our learned policy can zero-shot transfer to three different real robot settings: a fixed table-top Franka arm across two different labs, and an X-Arm on a mobile base, opening multiple unseen articulated objects across two labs, real lounges, and kitchens. Videos and code can be found on our project website: https://articubot.github.io/.</p></details> | Accepted at RSS 2025 |
| **[Quasi-Score Matching Estimation for Spatial Autoregressive Model with Random Weights Matrix and Regressors](http://arxiv.org/abs/2305.19721v2)** | 2025-05-01 | <details><summary>Show</summary><p>With the rapid advancements in technology for data collection, the application of the spatial autoregressive (SAR) model has become increasingly prevalent in real-world analysis, particularly when dealing with large datasets. However, the commonly used quasi-maximum likelihood estimation (QMLE) for the SAR model is not computationally scalable to handle the data with a large size. In addition, when establishing the asymptotic properties of the parameter estimators of the SAR model, both weights matrix and regressors are assumed to be nonstochastic in classical spatial econometrics, which is perhaps not realistic in real applications. Motivated by the machine learning literature, this paper proposes quasi-score matching estimation for the SAR model. This new estimation approach is developed based on the likelihood, but significantly reduces the computational complexity of the QMLE. The asymptotic properties of parameter estimators under the random weights matrix and regressors are established, which provides a new theoretical framework for the asymptotic inference of the SAR-type models. The usefulness of the quasi-score matching estimation and its asymptotic inference is illustrated via extensive simulation studies and a case study of an anti-conflict social network experiment for middle school students.</p></details> | 35 pages |
| **[Activation Steering in Neural Theorem Provers](http://arxiv.org/abs/2502.15507v2)** | 2025-05-01 | <details><summary>Show</summary><p>Large Language Models (LLMs) have shown promise in proving formal theorems using proof assistants like Lean. However, current state of the art language models struggles to predict next step in proofs leading practitioners to use different sampling techniques to improve LLMs capabilities. We observe that the LLM is capable of predicting the correct tactic; however, it faces challenges in ranking it appropriately within the set of candidate tactics, affecting the overall selection process. To overcome this hurdle, we use activation steering to guide LLMs responses to improve the generations at the time of inference. Our results suggest that activation steering offers a promising lightweight alternative to specialized fine-tuning for enhancing theorem proving capabilities in LLMs, particularly valuable in resource-constrained environments.</p></details> | <details><summary>incor...</summary><p>incorrect explanation for a concept, need to revise and update!</p></details> |
| **[Data-Driven Optical To Thermal Inference in Pool Boiling Using Generative Adversarial Networks](http://arxiv.org/abs/2505.00823v1)** | 2025-05-01 | <details><summary>Show</summary><p>Phase change plays a critical role in thermal management systems, yet quantitative characterization of multiphase heat transfer remains limited by the challenges of measuring temperature fields in chaotic, rapidly evolving flow regimes. While computational methods offer spatiotemporal resolution in idealized cases, replicating complex experimental conditions remains prohibitively difficult. Here, we present a data-driven framework that leverages a conditional generative adversarial network (CGAN) to infer temperature fields from geometric phase contours in a canonical pool boiling configuration where advanced data collection techniques are restricted. Using high-speed imaging data and simulation-informed training, our model demonstrates the ability to reconstruct temperature fields with errors below 6%. We further show that standard data augmentation strategies are effective in enhancing both accuracy and physical plausibility of the predicted maps across both simulation and experimental datasets when precise physical constraints are not applicable. Our results highlight the potential of deep generative models to bridge the gap between observable multiphase phenomena and underlying thermal transport, offering a powerful approach to augment and interpret experimental measurements in complex two-phase systems.</p></details> | <details><summary>17 pa...</summary><p>17 pages, 5 figures, supplemental information</p></details> |

