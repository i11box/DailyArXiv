---
title: Latest 6 Papers - October 08, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## Efficient Diffusion Models
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[TopInG: Topologically Interpretable Graph Learning via Persistent Rationale Filtration](http://arxiv.org/abs/2510.05102v1)** | 2025-10-06 | <details><summary>submi...</summary><p>submitted to ICML 2025</p></details> |
| **[Pulp Motion: Framing-aware multimodal camera and human motion generation](http://arxiv.org/abs/2510.05097v1)** | 2025-10-06 | <details><summary>Proje...</summary><p>Project page: https://www.lix.polytechnique.fr/vista/projects/2025_pulpmotion_courant/</p></details> |
| **[Paper2Video: Automatic Video Generation from Scientific Papers](http://arxiv.org/abs/2510.05096v1)** | 2025-10-06 | 20 pages, 8 figures |
| **[VChain: Chain-of-Visual-Thought for Reasoning in Video Generation](http://arxiv.org/abs/2510.05094v1)** | 2025-10-06 | <details><summary>Proje...</summary><p>Project page: https://eyeline-labs.github.io/VChain Code: https://github.com/Eyeline-Labs/VChain</p></details> |
| **[Learning to Interpret Weight Differences in Language Models](http://arxiv.org/abs/2510.05092v1)** | 2025-10-06 | <details><summary>The w...</summary><p>The weight diffs and DIT adapters trained in the paper can be found at https://huggingface.co/diff-interpretation-tuning/loras</p></details> |
| **[MALT: Improving Reasoning with Multi-Agent LLM Training](http://arxiv.org/abs/2412.01928v3)** | 2025-10-06 | <details><summary>Publi...</summary><p>Published at COLM 2025</p></details> |

## Transformer Compression
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Pulp Motion: Framing-aware multimodal camera and human motion generation](http://arxiv.org/abs/2510.05097v1)** | 2025-10-06 | <details><summary>Proje...</summary><p>Project page: https://www.lix.polytechnique.fr/vista/projects/2025_pulpmotion_courant/</p></details> |
| **[ResMimic: From General Motion Tracking to Humanoid Whole-body Loco-Manipulation via Residual Learning](http://arxiv.org/abs/2510.05070v1)** | 2025-10-06 | 9 pages, 8 figures |
| **[RowDetr: End-to-End Crop Row Detection Using Polynomials](http://arxiv.org/abs/2412.10525v3)** | 2025-10-06 | <details><summary>Code ...</summary><p>Code will be open sourced upon publication</p></details> |
| **[Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models](http://arxiv.org/abs/2510.05034v1)** | 2025-10-06 | The 1st version |
| **[Latent Uncertainty Representations for Video-based Driver Action and Intention Recognition](http://arxiv.org/abs/2510.05006v1)** | 2025-10-06 | <details><summary>16 pa...</summary><p>16 pages, 8 figures, 7 tables, under submission</p></details> |
| **[Power Transform Revisited: Numerically Stable, and Federated](http://arxiv.org/abs/2510.04995v1)** | 2025-10-06 | 25 pages |

## Fast Inference
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[VChain: Chain-of-Visual-Thought for Reasoning in Video Generation](http://arxiv.org/abs/2510.05094v1)** | 2025-10-06 | <details><summary>Proje...</summary><p>Project page: https://eyeline-labs.github.io/VChain Code: https://github.com/Eyeline-Labs/VChain</p></details> |
| **[Factuality Matters: When Image Generation and Editing Meet Structured Visuals](http://arxiv.org/abs/2510.05091v1)** | 2025-10-06 | <details><summary>Proje...</summary><p>Project page: https://structvisuals.github.io</p></details> |
| **[TeachLM: Post-Training LLMs for Education Using Authentic Learning Data](http://arxiv.org/abs/2510.05087v1)** | 2025-10-06 | 28 pages, 9 figures |
| **[RowDetr: End-to-End Crop Row Detection Using Polynomials](http://arxiv.org/abs/2412.10525v3)** | 2025-10-06 | <details><summary>Code ...</summary><p>Code will be open sourced upon publication</p></details> |
| **[Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models](http://arxiv.org/abs/2510.05034v1)** | 2025-10-06 | The 1st version |
| **[Curiosity-Driven Co-Development of Action and Language in Robots Through Self-Exploration](http://arxiv.org/abs/2510.05013v1)** | 2025-10-06 | <details><summary>26 pa...</summary><p>26 pages, 14 pages of supplementary material</p></details> |

