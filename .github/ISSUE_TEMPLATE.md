---
title: Latest 6 Papers - October 23, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## Efficient Diffusion Models
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs](http://arxiv.org/abs/2510.13795v2)** | 2025-10-21 | <details><summary>homep...</summary><p>homepage: https://open-bee.github.io/</p></details> |
| **[LightMem: Lightweight and Efficient Memory-Augmented Generation](http://arxiv.org/abs/2510.18866v1)** | 2025-10-21 | Work in progress |

## Transformer Compression
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[LightMem: Lightweight and Efficient Memory-Augmented Generation](http://arxiv.org/abs/2510.18866v1)** | 2025-10-21 | Work in progress |
| **[How Transformers Learn In-Context Recall Tasks? Optimality, Training Dynamics and Generalization](http://arxiv.org/abs/2505.15009v3)** | 2025-10-21 | <details><summary>V3: a...</summary><p>V3: added new results for softmax attention, typos fixed, titled changed. 33 pages</p></details> |
| **[FedDEAP: Adaptive Dual-Prompt Tuning for Multi-Domain Federated Learning](http://arxiv.org/abs/2510.18837v1)** | 2025-10-21 | Accepted at MM 2025 |
| **[Nondeterminism-Aware Optimistic Verification for Floating-Point Neural Networks](http://arxiv.org/abs/2510.16028v2)** | 2025-10-21 | 17 pages, 7 figures |
| **[Unifying and Enhancing Graph Transformers via a Hierarchical Mask Framework](http://arxiv.org/abs/2510.18825v1)** | 2025-10-21 | <details><summary>Accep...</summary><p>Accepted by NeurIPS 2025 (Poster)</p></details> |
| **[When LRP Diverges from Leave-One-Out in Transformers](http://arxiv.org/abs/2510.18810v1)** | 2025-10-21 | <details><summary>Black...</summary><p>BlackboxNLP @ EMNLP 2025</p></details> |

## Fast Inference
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[LightMem: Lightweight and Efficient Memory-Augmented Generation](http://arxiv.org/abs/2510.18866v1)** | 2025-10-21 | Work in progress |
| **[Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale Thinking Model](http://arxiv.org/abs/2510.18855v1)** | 2025-10-21 | Technical Report |
| **[Inference on Local Variable Importance Measures for Heterogeneous Treatment Effects](http://arxiv.org/abs/2510.18843v1)** | 2025-10-21 | 40 pages, 7 figures |
| **[How Transformers Learn In-Context Recall Tasks? Optimality, Training Dynamics and Generalization](http://arxiv.org/abs/2505.15009v3)** | 2025-10-21 | <details><summary>V3: a...</summary><p>V3: added new results for softmax attention, typos fixed, titled changed. 33 pages</p></details> |
| **[Nondeterminism-Aware Optimistic Verification for Floating-Point Neural Networks](http://arxiv.org/abs/2510.16028v2)** | 2025-10-21 | 17 pages, 7 figures |
| **[ShaRE your Data! Characterizing Datasets for LLM-based Requirements Engineering](http://arxiv.org/abs/2510.18787v1)** | 2025-10-21 | Under review |

