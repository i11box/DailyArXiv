---
title: Latest 6 Papers - May 23, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## Efficient Diffusion Models
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Streamline Without Sacrifice -- Squeeze out Computation Redundancy in LMM](http://arxiv.org/abs/2505.15816v1)** | 2025-05-21 | ICML 2025 |
| **[Leveraging the Powerful Attention of a Pre-trained Diffusion Model for Exemplar-based Image Colorization](http://arxiv.org/abs/2505.15812v1)** | 2025-05-21 | <details><summary>Accep...</summary><p>Accepted to IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</p></details> |
| **[On the creation of narrow AI: hierarchy and nonlocality of neural network skills](http://arxiv.org/abs/2505.15811v1)** | 2025-05-21 | 19 pages, 13 figures |
| **[MMaDA: Multimodal Large Diffusion Language Models](http://arxiv.org/abs/2505.15809v1)** | 2025-05-21 | <details><summary>Proje...</summary><p>Project: https://github.com/Gen-Verse/MMaDA</p></details> |
| **[Neural Conditional Transport Maps](http://arxiv.org/abs/2505.15808v1)** | 2025-05-21 | <details><summary>Under...</summary><p>Under Review. Supplementary material included in the pdf</p></details> |
| **[The Atlas of In-Context Learning: How Attention Heads Shape In-Context Retrieval Augmentation](http://arxiv.org/abs/2505.15807v1)** | 2025-05-21 | work in progress |

## Transformer Compression
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Adaptive Estimation and Learning under Temporal Distribution Shift](http://arxiv.org/abs/2505.15803v1)** | 2025-05-21 | <details><summary>Accep...</summary><p>Accepted at ICML 2025</p></details> |
| **[Interspatial Attention for Efficient 4D Human Video Generation](http://arxiv.org/abs/2505.15800v1)** | 2025-05-21 | <details><summary>Proje...</summary><p>Project page: https://dsaurus.github.io/isa4d/</p></details> |
| **[Exploring the Innovation Opportunities for Pre-trained Models](http://arxiv.org/abs/2505.15790v1)** | 2025-05-21 | <details><summary>33 pa...</summary><p>33 pages, 20 figures, 4 tables, DIS</p></details> |
| **[Towards Reliable and Interpretable Traffic Crash Pattern Prediction and Safety Interventions Using Customized Large Language Models](http://arxiv.org/abs/2505.12545v2)** | 2025-05-21 | <details><summary>Last ...</summary><p>Last revised 13 Feb 2025. Under review in Nature portfolio</p></details> |
| **[dMel: Speech Tokenization made Simple](http://arxiv.org/abs/2407.15835v3)** | 2025-05-21 | preprint |
| **[Long LEM Query in BWT-Runs Space](http://arxiv.org/abs/2505.15698v1)** | 2025-05-21 | 20 pages, 2 figures |

## Fast Inference
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Learning to Reason via Mixture-of-Thought for Logical Reasoning](http://arxiv.org/abs/2505.15817v1)** | 2025-05-21 | 38 pages |
| **[The Atlas of In-Context Learning: How Attention Heads Shape In-Context Retrieval Augmentation](http://arxiv.org/abs/2505.15807v1)** | 2025-05-21 | work in progress |
| **[Resource Heterogeneity-Aware and Utilization-Enhanced Scheduling for Deep Learning Clusters](http://arxiv.org/abs/2503.10918v2)** | 2025-05-21 | <details><summary>14 pa...</summary><p>14 pages, 12 figures, IEEE Transactions on Computers</p></details> |
| **[A Generative Diffusion Model to Solve Inverse Problems for Robust in-NICU Neonatal MRI](http://arxiv.org/abs/2410.21602v2)** | 2025-05-21 | <details><summary>6 pag...</summary><p>6 pages, 4 figures, submitted to ICIP 2025</p></details> |
| **[dKV-Cache: The Cache for Diffusion Language Models](http://arxiv.org/abs/2505.15781v1)** | 2025-05-21 | <details><summary>The c...</summary><p>The code is available at https://github.com/horseee/dKV-Cache</p></details> |
| **[Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: A Pseudo-Labeling and Unsupervised Learning Approach](http://arxiv.org/abs/2505.14449v2)** | 2025-05-21 | <details><summary>Accep...</summary><p>Accepted by InterSpeech 2025. 7 pages including 2 pages of appendix</p></details> |

