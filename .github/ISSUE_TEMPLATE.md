---
title: Latest 6 Papers - August 15, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## Efficient Diffusion Models
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion](http://arxiv.org/abs/2508.08241v3)** | 2025-08-13 | <details><summary>coin ...</summary><p>coin toss authorship, minor changes</p></details> |
| **[Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation](http://arxiv.org/abs/2508.09987v1)** | 2025-08-13 | 19 pages, 8 figures |
| **[Story2Board: A Training-Free Approach for Expressive Storyboard Generation](http://arxiv.org/abs/2508.09983v1)** | 2025-08-13 | <details><summary>Proje...</summary><p>Project page is available at https://daviddinkevich.github.io/Story2Board/</p></details> |
| **[RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache Compression](http://arxiv.org/abs/2502.14051v3)** | 2025-08-13 | ICML 2025 |
| **[Generalizing Scaling Laws for Dense and Sparse Large Language Models](http://arxiv.org/abs/2508.06617v2)** | 2025-08-13 | 8 pages, 8 figures |
| **[LLMC+: Benchmarking Vision-Language Model Compression with a Plug-and-play Toolkit](http://arxiv.org/abs/2508.09981v1)** | 2025-08-13 | 13 pages, 4 figures |

## Transformer Compression
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion](http://arxiv.org/abs/2508.08241v3)** | 2025-08-13 | <details><summary>coin ...</summary><p>coin toss authorship, minor changes</p></details> |
| **[RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache Compression](http://arxiv.org/abs/2502.14051v3)** | 2025-08-13 | ICML 2025 |
| **[LLMC+: Benchmarking Vision-Language Model Compression with a Plug-and-play Toolkit](http://arxiv.org/abs/2508.09981v1)** | 2025-08-13 | 13 pages, 4 figures |
| **[Leveraging Reviewer Experience in Code Review Comment Generation](http://arxiv.org/abs/2409.10959v2)** | 2025-08-13 | <details><summary>Accep...</summary><p>Accepted at ACM Transactions on Software Engineering and Methodology (TOSEM)</p></details> |
| **[MIND: A Noise-Adaptive Denoising Framework for Medical Images Integrating Multi-Scale Transformer](http://arxiv.org/abs/2508.07817v2)** | 2025-08-13 | <details><summary>Accep...</summary><p>Accepted by the 7th International Conference on Intelligent Control, Measurement and Signal Processing (ICMSP 2025). 6 pages, 6 figures</p></details> |
| **[Language of Persuasion and Misrepresentation in Business Communication: A Textual Detection Approach](http://arxiv.org/abs/2508.09935v1)** | 2025-08-13 | 21 |

## Fast Inference
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache Compression](http://arxiv.org/abs/2502.14051v3)** | 2025-08-13 | ICML 2025 |
| **[Vision-driven River Following of UAV via Safe Reinforcement Learning using Semantic Dynamics Model](http://arxiv.org/abs/2508.09971v1)** | 2025-08-13 | <details><summary>Submi...</summary><p>Submitted to Robotics and Autonomous Systems (RAS) journal</p></details> |
| **[Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models](http://arxiv.org/abs/2508.09968v1)** | 2025-08-13 | <details><summary>Proje...</summary><p>Project page: https://noisehypernetworks.github.io/</p></details> |
| **[GenAI Confessions: Black-box Membership Inference for Generative Image Models](http://arxiv.org/abs/2501.06399v2)** | 2025-08-13 | <details><summary>https...</summary><p>https://genai-confessions.github.io</p></details> |
| **[Block: Balancing Load in LLM Serving with Context, Knowledge and Predictive Scheduling](http://arxiv.org/abs/2508.03611v2)** | 2025-08-13 | <details><summary>12 pa...</summary><p>12 pages, 8 figures excluding appendix. V1: Fix some typos and grammar issue</p></details> |
| **[Stable Diffusion Models are Secretly Good at Visual In-Context Learning](http://arxiv.org/abs/2508.09949v1)** | 2025-08-13 | <details><summary>Accep...</summary><p>Accepted to ICCV 2025</p></details> |

